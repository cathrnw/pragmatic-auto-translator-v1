{
  "id": "section_2",
  "text": "BACKGROUND Similar to [14], we understand the term language model (LM) to refer to systems which are trained on string prediction tasks: that is, predicting the likelihood of a token (character, word or string) given either its preceding context or (in bidirectional and masked LMs) its surrounding context. Such systems are unsupervised and when deployed, take a text as input, commonly outputting scores or string predictions. Initially proposed by Shannon in 1949 [117], some of the earliest implemented LMs date to the early 1980s and were used as components in systems for automatic speech recognition (ASR), machine translation (MT), document classification, and more [111]. In this section, we provide a brief overview of the general trend of language modeling in recent years. For a more in-depth survey of pretrained LMs, see [105]. Citations: [[14]: ref_14] [[117]: ref_117] [[111]: ref_111] [[105]: ref_105] Before neural models, n-gram models also used large amounts of data [20, 87]. In addition to ASR, these large n-gram models of English were developed in the context of machine translation from another source language with far fewer direct translation examples. For example, [20] developed an n-gram model for English with a total of 1.8T n-grams and noted steady improvements in BLEU score on the test set of 1797 Arabic translations as the training data was increased from 13M tokens. Citations: [[20, 87]: ref_20, ref_87] [[20]: ref_20] The next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82]. Citations: [[85]: ref_85] [[98]: ref_98] [[82]: ref_82] [[99]: ref_99] [[99]: ref_99] [[e.g. 82]: ref_82] Transformer models, on the other hand, have been able to continuously benefit from larger architectures and larger quantities of data. Devlin et al. [39] in particular noted that training on a large dataset and fine-tuning for specific tasks leads to strictly increasing results on the GLUE tasks [138] for English as the hyperparameters of the model were increased. Initially developed as Chinese LMs, the ERNIE family [130, 131, 145] produced ERNIE-Gen, which was also trained on the original (English) BERT dataset, joining the ranks of very large LMs. NVIDIA released the MegatronLM which has 8.3B parameters and was trained on 174GB of text from the English Wikipedia, OpenWebText, RealNews and CC-Stories datasets [122]. Trained on the same dataset, Microsoft released T-NLG 1, an LM with 17B parameters. OpenAI's GPT-3 [25] and Google's GShard [73] and Switch-C [43] have increased the definition of large LM by orders of magnitude in terms of parameters at 175B, 600B, and 1.6T parameters, respectively. Table 1 summarizes a selection of these LMs in terms of training data size and parameters. As increasingly large amounts of text are collected from the web in datasets such as the Colossal Clean Crawled Corpus [107] and the Pile [51], this trend of increasingly large LMs can be expected to continue as long as they correlate with an increase in performance. [Footnote 1]: https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameterlanguage-model-by-microsoft/ Citations: [[39]: ref_39] [[138]: ref_138] [[130, 131, 145]: ref_130, ref_131, ref_145] [[122]: ref_122] [[25]: ref_25] [[73]: ref_73] [[43]: ref_43] [[107]: ref_107] [[51]: ref_51] [Ref: Table 1 -> table_1] A number of these models also have multilingual variants such as mBERT [39] and mT5 [148] or are trained with some amount of multilingual data such as GPT-3 where 7% of the training data was not in English [25]. The performance of these multilingual models across languages is an active area of research. Wu and Drezde [144] found that while mBERT does not perform equally well across all 104 languages in its training data, it performed better at NER, POS tagging, and dependency parsing than monolingual models trained with comparable amounts of data for four low-resource languages. Conversely, [95] surveyed monolingual BERT models developed with more specific architecture considerations or additional monolingual data and found that they generally outperform mBERT across 29 tasks. Either way, these models do not address the inclusion problems raised by [65], who note that over 90% of the world's languages used by more than a billion people currently have little to no support in terms of language technology. Citations: [[39]: ref_39] [[148]: ref_148] [[25]: ref_25] [[144]: ref_144] [[95]: ref_95] [[65]: ref_65] Alongside work investigating what information the models retain from the data, we see a trend in reducing the size of these models using various techniques such as knowledge distillation [26, 58], quantization [118, 153], factorized embedding parameterization and cross-layer parameter sharing [70], and progressive module replacing [146]. Rogers et al. [110] provide a comprehensive comparison of models derived from BERT using these techniques, such as DistilBERT [113] and ALBERT [70]. While these models maintain and sometimes exceed the performance of the original BERT model, despite their much smaller size, they ultimately still rely on large quantities of data and significant processing and storage capabilities to both hold and reduce the model. Citations: [[26, 58]: ref_26, ref_58] [[118, 153]: ref_118, ref_153] [[70]: ref_70] [[146]: ref_146] [[110]: ref_110] [[113]: ref_113] [[70]: ref_70] We note that the change from n-gram LMs to word vectors distilled from neural LMs to pretrained Transformer LMs is paralleled by an expansion and change in the types of tasks they are useful for: n-gram LMs were initially typically deployed in selecting among the outputs of e.g. acoustical or translation models; the LSTM-derived word vectors were quickly picked up as more effective representations of words (in place of bag of words features) in a variety of NLP tasks involving labeling and classification; and the pretrained Transformer models can be retrained on very small datasets (few-shot, one-shot or even zero-shot learning) to perform apparently meaning-manipulating tasks such as summarization, question answering and the like. Nonetheless, all of these systems share the property of being LMs in the sense we give above, that is, systems trained to predict sequences of words (or characters or sentences). Where they differ is in the size of the training datasets they leverage and the spheres of influence they can possibly affect. By scaling up in these two ways, modern very large LMs incur new kinds of risk, which we turn to in the following sections.",
  "embedding": [
    0.111328125,
    -0.05419921875,
    -0.0013275146484375,
    0.0260009765625,
    -0.01025390625,
    0.023193359375,
    -0.162109375,
    -0.031494140625,
    -0.056884765625,
    -0.1572265625,
    -0.0791015625,
    0.1162109375,
    0.004058837890625,
    -0.049072265625,
    0.0296630859375,
    0.038818359375,
    -0.05224609375,
    -0.05419921875,
    -0.18359375,
    -0.0179443359375,
    -0.035400390625,
    0.0283203125,
    0.11083984375,
    0.0263671875,
    0.024658203125,
    0.0595703125,
    -0.091796875,
    -0.0859375,
    0.01324462890625,
    -0.01446533203125,
    0.031982421875,
    -0.051513671875,
    0.044921875,
    0.020751953125,
    0.05078125,
    -0.0291748046875,
    0.07958984375,
    0.01953125,
    -0.1240234375,
    -0.0322265625,
    0.027099609375,
    0.003936767578125,
    -0.03564453125,
    0.00506591796875,
    0.04345703125,
    0.0247802734375,
    0.005279541015625,
    -0.0140380859375,
    0.023193359375,
    0.0771484375,
    -0.042724609375,
    0.01708984375,
    -0.05615234375,
    0.033203125,
    -0.03857421875,
    -0.002716064453125,
    0.0400390625,
    0.003753662109375,
    0.0439453125,
    0.06201171875,
    -0.08935546875,
    0.036865234375,
    0.07763671875,
    -0.04150390625,
    0.034423828125,
    0.010498046875,
    -0.009033203125,
    -0.055908203125,
    0.07861328125,
    -0.041748046875,
    -0.0341796875,
    0.0205078125,
    0.045166015625,
    0.080078125,
    -0.1240234375,
    -0.0015716552734375,
    0.013427734375,
    0.027099609375,
    -0.0206298828125,
    -0.0164794921875,
    0.051513671875,
    -0.045654296875,
    -0.0150146484375,
    0.0030517578125,
    0.006591796875,
    0.0159912109375,
    0.0751953125,
    -0.02880859375,
    0.0159912109375,
    0.053466796875,
    -0.0245361328125,
    0.01171875,
    -0.0498046875,
    0.01043701171875,
    -0.020263671875,
    -0.0147705078125,
    -0.0194091796875,
    0.049560546875,
    -0.018310546875,
    -0.0576171875,
    0.052978515625,
    0.052734375,
    -0.0771484375,
    0.06982421875,
    -0.00787353515625,
    -0.002593994140625,
    0.0118408203125,
    0.06884765625,
    -0.03369140625,
    -0.02294921875,
    -0.0927734375,
    -0.00628662109375,
    -0.04833984375,
    0.0185546875,
    -0.034423828125,
    0.049072265625,
    -0.04345703125,
    -0.032470703125,
    0.039794921875,
    -0.0135498046875,
    0.00160980224609375,
    -0.0291748046875,
    0.0194091796875,
    0.050048828125,
    0.0111083984375,
    0.017822265625,
    -0.038818359375,
    -0.0654296875,
    -0.0185546875,
    -0.05859375,
    0.0162353515625,
    0.07080078125,
    -0.037353515625,
    0.0810546875,
    0.02978515625,
    -0.0322265625,
    0.01324462890625,
    0.0233154296875,
    0.0234375,
    0.0028228759765625,
    -0.033447265625,
    0.046142578125,
    0.006256103515625,
    -0.050537109375,
    -0.0123291015625,
    -0.04931640625,
    0.039306640625,
    -0.004669189453125,
    -0.00921630859375,
    -0.052001953125,
    0.0166015625,
    0.052734375,
    -0.020263671875,
    0.0888671875,
    -0.052490234375,
    -0.06884765625,
    -0.015625,
    -0.012939453125,
    -0.029296875,
    0.0123291015625,
    -0.0234375,
    0.031982421875,
    -0.0010223388671875,
    0.02734375,
    -0.03369140625,
    0.07275390625,
    -0.0230712890625,
    0.019287109375,
    0.0218505859375,
    0.04150390625,
    0.003814697265625,
    -0.03759765625,
    0.039306640625,
    -0.0458984375,
    -0.01220703125,
    -0.0302734375,
    -0.0233154296875,
    -0.010498046875,
    -0.0211181640625,
    0.036376953125,
    -0.00537109375,
    0.0252685546875,
    -0.0164794921875,
    -0.030517578125,
    -0.0186767578125,
    -0.024169921875,
    -0.025634765625,
    -0.00970458984375,
    -0.019287109375,
    -0.0302734375,
    0.07666015625,
    0.0255126953125,
    0.07373046875,
    -0.0023345947265625,
    0.059326171875,
    -0.0341796875,
    0.005828857421875,
    -0.040771484375,
    -0.032958984375,
    0.011962890625,
    -0.059326171875,
    -0.034423828125,
    -0.00003409385681152344,
    -0.00531005859375,
    -0.0076904296875,
    -0.01171875,
    -0.0216064453125,
    -0.00750732421875,
    -0.0201416015625,
    0.045654296875,
    -0.004302978515625,
    0.01220703125,
    0.048095703125,
    0.0264892578125,
    -0.01068115234375,
    -0.0196533203125,
    0.05322265625,
    0.04296875,
    0.0830078125,
    0.01165771484375,
    0.0272216796875,
    -0.0234375,
    0.0269775390625,
    0.0291748046875,
    0.03759765625,
    -0.03515625,
    0.03173828125,
    0.05517578125,
    -0.0238037109375,
    0.048583984375,
    -0.0224609375,
    0.050048828125,
    -0.00628662109375,
    -0.06640625,
    -0.00799560546875,
    0.00665283203125,
    -0.0074462890625,
    0.035400390625,
    0.0693359375,
    -0.02490234375,
    0.0208740234375,
    0.01361083984375,
    -0.0267333984375,
    0.0218505859375,
    -0.03955078125,
    0.038818359375,
    0.00689697265625,
    -0.0087890625,
    -0.0179443359375,
    0.01220703125,
    0.0025482177734375,
    -0.048583984375,
    -0.05517578125,
    0.0023193359375,
    -0.0196533203125,
    0.0020599365234375,
    0.0130615234375,
    -0.00897216796875,
    0.00921630859375,
    0.01513671875,
    0.0390625,
    -0.00750732421875,
    0.0177001953125,
    0.0034942626953125,
    -0.026611328125,
    0.0634765625,
    -0.00141143798828125,
    0.0166015625,
    -0.01220703125,
    -0.04248046875,
    -0.0213623046875,
    -0.039306640625,
    -0.0015411376953125,
    -0.0034027099609375,
    0.022216796875,
    -0.01434326171875,
    -0.031982421875,
    -0.00518798828125,
    -0.0361328125,
    0.0166015625,
    -0.054931640625,
    -0.0089111328125,
    0.056884765625,
    -0.0205078125,
    0.0196533203125,
    -0.00701904296875,
    -0.00469970703125,
    0.00994873046875,
    0.01434326171875,
    -0.0103759765625,
    -0.00762939453125,
    -0.00457763671875,
    0.0091552734375,
    -0.031494140625,
    -0.00701904296875,
    0.014404296875,
    -0.046875,
    0.0028228759765625,
    0.06494140625,
    0.0291748046875,
    -0.04052734375,
    0.037109375,
    -0.010986328125,
    0.0625,
    0.03271484375,
    0.0177001953125,
    0.01483154296875,
    0.017578125,
    -0.028564453125,
    0.01312255859375,
    -0.01214599609375,
    -0.015625,
    -0.0142822265625,
    0.0289306640625,
    -0.0233154296875,
    0.031494140625,
    0.028076171875,
    0.005523681640625,
    -0.0380859375,
    -0.016845703125,
    0.05126953125,
    0.0032806396484375,
    0.006591796875,
    -0.04345703125,
    0.01483154296875,
    -0.049072265625,
    0.02099609375,
    -0.0238037109375,
    0.027587890625,
    -0.00592041015625,
    -0.00970458984375,
    0.003753662109375,
    0.0264892578125,
    0.0157470703125,
    0.01055908203125,
    -0.041748046875,
    -0.04833984375,
    -0.019287109375,
    0.0272216796875,
    -0.00811767578125,
    0.00982666015625,
    0.017578125,
    -0.00238037109375,
    0.003753662109375,
    -0.048828125,
    0.046875,
    0.029052734375,
    0.022705078125,
    0.003753662109375,
    -0.0174560546875,
    0.02734375,
    0.0247802734375,
    -0.0029144287109375,
    0.00506591796875,
    -0.006744384765625,
    -0.004058837890625,
    -0.060546875,
    0.0234375,
    0.05029296875,
    -0.0186767578125,
    -0.044677734375,
    -0.006591796875,
    -0.03564453125,
    0.054931640625,
    -0.0172119140625,
    0.0289306640625,
    0.006683349609375,
    0.058349609375,
    -0.00162506103515625,
    -0.014892578125,
    0.009765625,
    0.025390625,
    0.037841796875,
    -0.01141357421875,
    -0.04541015625,
    -0.012451171875,
    0.036376953125,
    -0.007293701171875,
    0.04931640625,
    0.01092529296875,
    0.0079345703125,
    0.0242919921875,
    0.019287109375,
    0.030029296875,
    0.0159912109375,
    0.0030364990234375,
    0.00384521484375,
    -0.007568359375,
    -0.0096435546875,
    0.0595703125,
    -0.0230712890625,
    0.04248046875,
    0.025634765625,
    0.00811767578125,
    -0.0277099609375,
    0.00142669677734375,
    0.00135040283203125,
    0.048095703125,
    0.019775390625,
    0.00946044921875,
    0.00189208984375,
    -0.020263671875,
    0.012939453125,
    -0.000301361083984375,
    -0.0216064453125,
    0.021240234375,
    0.0135498046875,
    0.0233154296875,
    0.00775146484375,
    -0.042724609375,
    -0.083984375,
    -0.03564453125,
    -0.04833984375,
    0.015869140625,
    0.008056640625,
    -0.03857421875,
    -0.014892578125,
    0.0419921875,
    -0.040283203125,
    0.017822265625,
    -0.007415771484375,
    -0.018798828125,
    0.002349853515625,
    0.000263214111328125,
    -0.0230712890625,
    -0.039794921875,
    -0.043701171875,
    -0.0145263671875,
    -0.015869140625,
    0.02978515625,
    0.00762939453125,
    -0.005889892578125,
    0.0206298828125,
    0.0233154296875,
    -0.0244140625,
    0.00323486328125,
    0.00775146484375,
    0.00127410888671875,
    -0.0024566650390625,
    0.02294921875,
    0.0390625,
    0.03515625,
    0.032470703125,
    0.0010833740234375,
    0.033203125,
    0.0091552734375,
    -0.01019287109375,
    -0.03759765625,
    -0.0198974609375,
    0.042236328125,
    0.0189208984375,
    0.04150390625,
    -0.016357421875,
    -0.03076171875,
    -0.04150390625,
    -0.018798828125,
    0.039306640625,
    -0.032958984375,
    0.0045166015625,
    0.0244140625,
    0.027099609375,
    -0.041748046875,
    0.05322265625,
    -0.04052734375,
    0.08984375,
    0.0302734375,
    0.01171875,
    0.0016632080078125,
    0.0216064453125,
    -0.043212890625,
    0.006561279296875,
    0.037841796875,
    0.015380859375,
    -0.0244140625,
    -0.06787109375,
    0.0076904296875,
    -0.0016632080078125,
    -0.01348876953125,
    0.00445556640625,
    -0.03662109375,
    -0.054931640625,
    -0.041015625,
    -0.006591796875,
    0.03759765625,
    0.0142822265625,
    0.000339508056640625,
    0.056884765625,
    0.0185546875,
    0.025390625,
    -0.0189208984375,
    -0.021728515625,
    -0.0177001953125,
    0.0169677734375,
    0.0009307861328125,
    -0.0062255859375,
    0.03076171875,
    -0.046630859375,
    0.0023956298828125,
    -0.051025390625,
    0.01141357421875,
    -0.01171875,
    0.01422119140625,
    -0.035888671875,
    0.004852294921875,
    -0.033935546875,
    -0.025634765625,
    0.00077056884765625,
    0.0220947265625,
    0.0068359375,
    0.01275634765625,
    -0.01165771484375,
    0.00860595703125,
    0.046630859375,
    0.000522613525390625,
    -0.0091552734375,
    -0.0184326171875,
    0.0135498046875,
    -0.003997802734375,
    -0.0238037109375,
    0.029296875,
    -0.03759765625,
    -0.00238037109375,
    -0.01312255859375,
    -0.0135498046875,
    0.004180908203125,
    -0.029052734375,
    -0.041015625,
    0.033447265625,
    0.0174560546875,
    0.00457763671875,
    0.03857421875,
    0.033203125,
    -0.0296630859375,
    -0.004791259765625,
    0.0240478515625,
    0.001922607421875,
    -0.004638671875,
    -0.053466796875,
    -0.0155029296875,
    -0.0035400390625,
    0.01171875,
    0.00213623046875,
    -0.001861572265625,
    -0.005523681640625,
    0.002716064453125,
    -0.008544921875,
    -0.00958251953125,
    0.017578125,
    -0.0235595703125,
    0.01300048828125,
    0.00372314453125,
    0.00726318359375,
    0.0081787109375,
    0.034423828125,
    -0.00885009765625,
    0.0118408203125,
    -0.0047607421875,
    -0.0242919921875,
    0.026611328125,
    -0.024658203125,
    0.0174560546875,
    0.00830078125,
    -0.004669189453125,
    -0.020751953125,
    0.0033721923828125,
    -0.004241943359375,
    -0.0177001953125,
    -0.02001953125,
    -0.0076904296875,
    0.0255126953125,
    -0.0025787353515625,
    0.00067138671875,
    0.006317138671875,
    -0.003326416015625,
    -0.00439453125,
    -0.00103759765625,
    -0.026611328125,
    0.0213623046875,
    -0.026611328125,
    0.019775390625,
    0.0093994140625,
    -0.01300048828125,
    -0.0283203125,
    0.00147247314453125,
    -0.031982421875,
    0.0020751953125,
    0.03759765625,
    0.0250244140625,
    -0.0155029296875,
    0.0072021484375,
    -0.002166748046875,
    -0.04541015625,
    -0.015625,
    -0.0189208984375,
    0.07568359375,
    -0.0174560546875,
    -0.030517578125,
    -0.044189453125,
    -0.00138092041015625,
    -0.04248046875,
    -0.01190185546875,
    -0.022216796875,
    -0.01434326171875,
    -0.006103515625,
    -0.035888671875,
    0.030029296875,
    0.0240478515625,
    0.007171630859375,
    0.0223388671875,
    -0.015625,
    0.0074462890625,
    0.0028533935546875,
    0.0234375,
    0.0201416015625,
    0.0072021484375,
    0.007293701171875,
    0.00347900390625,
    0.0284423828125,
    -0.0252685546875,
    -0.03662109375,
    -0.0286865234375,
    -0.01080322265625,
    0.0245361328125,
    -0.0078125,
    0.01019287109375,
    -0.0155029296875,
    -0.0244140625,
    0.0546875,
    -0.033447265625,
    0.024169921875,
    -0.0140380859375,
    0.004608154296875,
    0.0107421875,
    0.000043392181396484375,
    0.0069580078125,
    0.0089111328125,
    0.06982421875,
    0.004058837890625,
    0.046142578125,
    -0.01275634765625,
    0.00933837890625,
    -0.0137939453125,
    0.0157470703125,
    0.006134033203125,
    0.00762939453125,
    -0.0286865234375,
    0.0047607421875,
    0.01422119140625,
    -0.0137939453125,
    0.003173828125,
    -0.0284423828125,
    -0.041748046875,
    -0.00323486328125,
    0.03076171875,
    -0.0167236328125,
    -0.03759765625,
    0.00897216796875,
    0.04296875,
    -0.0216064453125,
    -0.028076171875,
    0.0169677734375,
    -0.0252685546875,
    0.0255126953125,
    -0.0089111328125,
    -0.0196533203125,
    0.034912109375,
    0.0169677734375,
    0.0238037109375,
    -0.0169677734375,
    0.0002918243408203125,
    -0.021484375,
    0.0086669921875,
    0.052734375,
    -0.03125,
    0.03271484375,
    0.00787353515625,
    0.03125,
    0.0047607421875,
    0.033203125,
    -0.00909423828125,
    0.0189208984375,
    0.0166015625,
    -0.0213623046875,
    0.031494140625,
    -0.009521484375,
    -0.01239013671875,
    -0.005706787109375,
    0.0263671875,
    -0.006103515625,
    0.0264892578125,
    -0.001190185546875,
    0.0189208984375,
    -0.0086669921875,
    -0.0028076171875,
    0.0033111572265625,
    0.005523681640625,
    0.03369140625,
    0.04052734375,
    -0.03369140625,
    -0.007415771484375,
    -0.0179443359375,
    0.045166015625,
    0.036865234375,
    0.003875732421875,
    -0.00860595703125,
    -0.0205078125,
    0.0002880096435546875,
    0.00396728515625,
    0.0023651123046875,
    0.0098876953125,
    0.00823974609375,
    -0.045166015625,
    0.01483154296875,
    0.006988525390625,
    0.01397705078125,
    -0.00531005859375,
    -0.00494384765625,
    -0.06689453125,
    0.00013256072998046875,
    0.04052734375,
    -0.04638671875,
    -0.0147705078125,
    0.021484375,
    0.025390625,
    -0.004364013671875,
    0.024658203125,
    0.036376953125,
    -0.0079345703125,
    -0.01055908203125,
    -0.0118408203125,
    0.010498046875,
    -0.01080322265625,
    -0.01226806640625,
    -0.030029296875,
    -0.03173828125,
    -0.01513671875,
    0.01287841796875,
    0.020263671875,
    -0.02001953125,
    -0.05419921875,
    0.0166015625,
    0.0096435546875,
    -0.0260009765625,
    0.01513671875,
    0.0172119140625,
    -0.03173828125,
    0.0322265625,
    -0.0277099609375,
    0.00946044921875,
    -0.0390625,
    0.0048828125,
    0.0203857421875,
    0.017333984375,
    -0.0213623046875,
    0.01092529296875,
    -0.01165771484375,
    -0.00860595703125,
    0.0078125,
    -0.01068115234375,
    0.000823974609375,
    -0.0203857421875,
    0.042724609375,
    0.03125,
    0.053955078125,
    -0.036865234375,
    0.0546875,
    0.0478515625,
    -0.017333984375,
    0.0257568359375,
    0.0252685546875,
    0.036376953125,
    -0.0069580078125,
    -0.04248046875,
    0.028564453125,
    0.02099609375,
    -0.0191650390625,
    0.013671875,
    0.0123291015625,
    -0.0247802734375,
    0.03173828125,
    -0.006988525390625,
    -0.034423828125,
    -0.0255126953125,
    0.04248046875,
    0.0240478515625,
    -0.006195068359375,
    -0.01422119140625,
    0.0244140625,
    -0.0439453125,
    0.0020751953125,
    -0.01153564453125,
    -0.006378173828125,
    -0.0250244140625,
    0.033935546875,
    -0.0157470703125,
    0.03125,
    -0.025146484375,
    0.02734375,
    -0.0118408203125,
    -0.015380859375,
    -0.002716064453125,
    0.0076904296875,
    -0.0140380859375,
    0.0264892578125,
    0.030517578125,
    0.0152587890625,
    -0.01361083984375,
    -0.0057373046875,
    0.03857421875,
    0.005584716796875,
    0.0181884765625,
    0.0014495849609375,
    -0.00634765625,
    0.0054931640625,
    -0.010986328125,
    0.0029296875,
    0.0029754638671875,
    0.00506591796875,
    -0.03857421875,
    -0.0380859375,
    -0.006500244140625,
    0.00897216796875,
    -0.00238037109375,
    -0.00958251953125,
    -0.05859375,
    0.0172119140625,
    0.030517578125,
    -0.0019989013671875,
    -0.045654296875,
    0.0208740234375,
    -0.010498046875,
    0.0096435546875,
    -0.031005859375,
    -0.005859375,
    -0.021484375,
    -0.0091552734375,
    -0.01611328125,
    0.0072021484375,
    0.00323486328125,
    0.01239013671875,
    -0.0220947265625,
    0.000885009765625,
    0.022216796875,
    0.0026092529296875,
    -0.0302734375,
    0.031982421875,
    -0.03173828125,
    -0.0225830078125,
    0.005340576171875,
    0.01318359375,
    -0.02783203125,
    -0.025634765625,
    -0.003143310546875,
    0.00075531005859375,
    -0.01092529296875,
    0.0142822265625,
    0.014892578125,
    -0.03759765625,
    -0.0296630859375,
    -0.01611328125,
    -0.0233154296875,
    0.030029296875,
    -0.0244140625,
    -0.011474609375,
    -0.012451171875,
    -0.0206298828125,
    -0.01513671875,
    0.00095367431640625,
    0.0272216796875,
    0.0211181640625,
    0.0255126953125,
    -0.0230712890625,
    -0.0033111572265625,
    -0.041015625,
    -0.01318359375,
    -0.0206298828125,
    -0.0322265625,
    -0.02099609375,
    -0.029052734375,
    0.036376953125,
    -0.0302734375,
    0.015625,
    0.01080322265625,
    0.044677734375,
    0.0294189453125,
    0.03759765625,
    -0.01708984375,
    -0.004913330078125,
    -0.0380859375,
    -0.00018787384033203125,
    -0.0167236328125,
    -0.00970458984375,
    -0.0291748046875,
    0.0260009765625,
    -0.0093994140625,
    0.0035247802734375,
    -0.03466796875,
    -0.0150146484375,
    0.01214599609375,
    -0.019775390625,
    0.01019287109375,
    0.0072021484375,
    0.0101318359375,
    0.0155029296875,
    0.018310546875,
    0.020263671875,
    0.030029296875,
    0.0027618408203125,
    0.008056640625,
    -0.0230712890625,
    0.0174560546875,
    0.055419921875,
    -0.0172119140625,
    0.037109375,
    -0.0228271484375,
    0.0255126953125,
    0.005279541015625,
    0.0050048828125,
    -0.0595703125,
    0.00469970703125,
    -0.010009765625,
    0.0101318359375,
    0.000728607177734375,
    -0.057861328125,
    -0.0361328125,
    -0.0159912109375,
    0.0108642578125,
    -0.036376953125,
    -0.0184326171875,
    -0.025390625,
    -0.00543212890625,
    0.00640869140625,
    0.07275390625,
    -0.027587890625,
    -0.003875732421875,
    -0.005035400390625,
    0.0198974609375,
    -0.0242919921875,
    -0.013671875,
    0.0250244140625,
    0.01312255859375,
    0.0093994140625,
    0.01043701171875,
    0.0203857421875,
    0.00860595703125,
    0.037841796875,
    0.0003833770751953125,
    -0.0081787109375,
    0.0045166015625,
    0.0224609375,
    -0.00018787384033203125,
    0.001190185546875,
    -0.0101318359375,
    0.00012874603271484375,
    -0.0164794921875,
    -0.0152587890625,
    0.04052734375,
    0.01458740234375,
    0.005615234375,
    -0.0185546875,
    -0.00848388671875,
    0.0201416015625,
    -0.0269775390625,
    0.019775390625,
    -0.025634765625,
    0.011962890625,
    0.026123046875,
    0.01806640625,
    0.0250244140625,
    0.03173828125,
    0.0106201171875,
    -0.00933837890625,
    -0.0186767578125,
    0.01104736328125,
    0.019775390625,
    -0.01141357421875,
    0.02294921875,
    -0.00101470947265625,
    -0.011962890625,
    0.00909423828125,
    0.0101318359375,
    -0.00958251953125,
    0.03125,
    -0.016357421875,
    0.015380859375,
    0.017333984375,
    -0.006317138671875,
    -0.001953125,
    -0.0052490234375,
    0.0216064453125,
    0.0111083984375,
    -0.028076171875,
    -0.0264892578125,
    0.01080322265625,
    -0.00482177734375,
    -0.00225830078125,
    -0.00567626953125,
    -0.005889892578125,
    0.01202392578125,
    -0.03955078125,
    -0.022216796875,
    -0.031982421875,
    0.0123291015625,
    -0.0062255859375,
    0.0018768310546875,
    -0.02197265625,
    -0.03271484375,
    -0.003509521484375,
    -0.001861572265625,
    0.0098876953125,
    0.036865234375,
    -0.0072021484375,
    0.0255126953125,
    0.0172119140625,
    -0.0096435546875,
    0.01611328125,
    0.0133056640625,
    0.0087890625,
    -0.0087890625,
    -0.010009765625,
    -0.00885009765625,
    0.0203857421875,
    0.0135498046875,
    0.0081787109375
  ],
  "created": "2025-06-12T05:14:14.234978",
  "count": 25
}