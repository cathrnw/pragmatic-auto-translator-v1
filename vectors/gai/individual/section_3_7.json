{
  "id": "section_3",
  "text": "III. VALUES AND PRINCIPLES 9. The values and principles included below should be respected by all actors in the AI system life cycle, in the first place and, where needed and appropriate, be promoted through amendments to the existing and elaboration of new legislation, regulations and business guidelines. This must comply with international law, including the United Nations Charter and Member States' human rights obligations, and should be in line with internationally agreed social, political, environmental, educational, scientific and economic sustainability objectives, such as the United Nations Sustainable Development Goals (SDGs). 10. Values play a powerful role as motivating ideals in shaping policy measures and legal norms. While the set of values outlined below thus inspires desirable behaviour and represents the foundations of principles, the principles unpack the values underlying them more concretely so that the values can be more easily operationalized in policy statements and actions. 11. While all the values and principles outlined below are desirable per se, in any practical contexts, there may be tensions between these values and principles. In any given situation, a contextual assessment will be necessary to manage potential tensions, taking into account the principle of proportionality and in compliance with human rights and fundamental freedoms. In all cases, any possible limitations on human rights and fundamental freedoms must have a lawful basis, and be reasonable, necessary and proportionate, and consistent with States' obligations under international law. To navigate such scenarios judiciously will typically require engagement with a broad range of appropriate stakeholders, making use of social dialogue, as well as ethical deliberation, due diligence and impact assessment. 12. The trustworthiness and integrity of the life cycle of AI systems is essential to ensure that AI technologies will work for the good of humanity, individuals, societies and the environment and ecosystems, and embody the values and principles set out in this Recommendation. People should have good reason to trust that AI systems can bring individual and shared benefits, while adequate measures are taken to mitigate risks. An essential requirement for trustworthiness is that, throughout their life cycle, AI systems are subject to thorough monitoring by the relevant stakeholders as appropriate. As trustworthiness is an outcome of the operationalization of the principles in this document, the policy actions proposed in this Recommendation are all directed at promoting trustworthiness in all stages of the AI system life cycle. III.1 VALUES Respect, protection and promotion of human rights and fundamental freedoms and human dignity 13. The inviolable and inherent dignity of every human constitutes the foundation for the universal, indivisible, inalienable, interdependent and interrelated system of human rights and fundamental freedoms. Therefore, respect, protection and promotion of human dignity and rights as established by international law, including international human rights law, is essential throughout the life cycle of AI systems. Human dignity relates to the recognition of the intrinsic and equal worth of each individual human being, regardless of race, colour, descent, gender, age, language, religion, political opinion, national origin, ethnic origin, social origin, economic or social condition of birth, or disability and any other grounds. 14. No human being or human community should be harmed or subordinated, whether physically, economically, socially, politically, culturally or mentally during any phase of the life cycle of AI systems. Throughout the life cycle of AI systems, the quality of life of human beings should be enhanced, while the definition of 'quality of life' should be left open to individuals or groups, as long as there is no violation or abuse of human rights and fundamental freedoms, or the dignity of humans in terms of this definition. 15. Persons may interact with AI systems throughout their life cycle and receive assistance from them, such as care for vulnerable people or people in vulnerable situations, including but not limited to children, older persons, persons with disabilities or the ill. Within such interactions, persons should never be objectified, nor should their dignity be otherwise undermined, or human rights and fundamental freedoms violated or abused. 16. Human rights and fundamental freedoms must be respected, protected and promoted throughout the life cycle of AI systems. Governments, private sector, civil society, international organizations, technical communities and academia must respect human rights instruments and frameworks in their interventions in the processes surrounding the life cycle of AI systems. New technologies need to provide new means to advocate, defend and exercise human rights and not to infringe them. Environmental and ecosystem flourishing 17. Environmental and ecosystem flourishing should be recognized, protected and promoted through the life cycle of AI systems. Furthermore, environment and ecosystems are the existential necessity for humanity and other living beings to be able to enjoy the benefits of advances in AI. 18. All actors involved in the life cycle of AI systems must comply with applicable international law and domestic legislation, standards and practices, such as precaution, designed for environmental and ecosystem protection and restoration, and sustainable development. They should reduce the environmental impact of AI systems, including but not limited to its carbon footprint, to ensure the minimization of climate change and environmental risk factors, and prevent the unsustainable exploitation, use and transformation of natural resources contributing to the deterioration of the environment and the degradation of ecosystems. Ensuring diversity and inclusiveness 19. Respect, protection and promotion of diversity and inclusiveness should be ensured throughout the life cycle of AI systems, consistent with international law, including human rights law. This may be done by promoting active participation of all individuals or groups regardless of race, colour, descent, gender, age, language, religion, political opinion, national origin, ethnic origin, social origin, economic or social condition of birth, or disability and any other grounds. 20. The scope of lifestyle choices, beliefs, opinions, expressions or personal experiences, including the optional use of AI systems and the co-design of these architectures should not be restricted during any phase of the life cycle of AI systems. 21. Furthermore, efforts, including international cooperation, should be made to overcome, and never take advantage of, the lack of necessary technological infrastructure, education and skills, as well as legal frameworks, particularly in LMICs, LDCs, LLDCs and SIDS, affecting communities. Living in peaceful, just and interconnected societies 22. AI actors should play a participative and enabling role to ensure peaceful and just societies, which is based on an interconnected future for the benefit of all, consistent with human rights and fundamental freedoms. The value of living in peaceful and just societies points to the potential of AI systems to contribute throughout their life cycle to the interconnectedness of all living creatures with each other and with the natural environment. 23. The notion of humans being interconnected is based on the knowledge that every human belongs to a greater whole, which thrives when all its constituent parts are enabled to thrive. Living in peaceful, just and interconnected societies requires an organic, immediate, uncalculated bond of solidarity, characterized by a permanent search for peaceful relations, tending towards care for others and the natural environment in the broadest sense of the term. 24. This value demands that peace, inclusiveness and justice, equity and interconnectedness should be promoted throughout the life cycle of AI systems, in so far as the processes of the life cycle of AI systems should not segregate, objectify or undermine freedom and autonomous decision-making as well as the safety of human beings and communities, divide and turn individuals and groups against each other, or threaten the coexistence between humans, other living beings and the natural environment. III.2 PRINCIPLES Proportionality and Do No Harm 25. It should be recognized that AI technologies do not necessarily, per se, ensure human and environmental and ecosystem flourishing. Furthermore, none of the processes related to the AI system life cycle shall exceed what is necessary to achieve legitimate aims or objectives and should be appropriate to the context. In the event of possible occurrence of any harm to human beings, human rights and fundamental freedoms, communities and society at large or the environment and ecosystems, the implementation of procedures for risk assessment and the adoption of measures in order to preclude the occurrence of such harm should be ensured. 26. The choice to use AI systems and which AI method to use should be justified in the following ways: (a) the AI method chosen should be appropriate and proportional to achieve a given legitimate aim; (b) the AI method chosen should not infringe upon the foundational values captured in this document, in particular, its use must not violate or abuse human rights; and (c) the AI method should be appropriate to the context and should be based on rigorous scientific foundations. In scenarios where decisions are understood to have an impact that is irreversible or difficult to reverse or may involve life and death decisions, final human determination should apply. In particular, AI systems should not be used for social scoring or mass surveillance purposes. Safety and security 27. Unwanted harms (safety risks), as well as vulnerabilities to attack (security risks) should be avoided and should be addressed, prevented and eliminated throughout the life cycle of AI systems to ensure human, environmental and ecosystem safety and security. Safe and secure AI will be enabled by the development of sustainable, privacy-protective data access frameworks that foster better training and validation of AI models utilizing quality data. Fairness and non-discrimination 28. AI actors should promote social justice and safeguard fairness and non-discrimination of any kind in compliance with international law. This implies an inclusive approach to ensuring that the benefits of AI technologies are available and accessible to all, taking into consideration the specific needs of different age groups, cultural systems, different language groups, persons with disabilities, girls and women, and disadvantaged, marginalized and vulnerable people or people in vulnerable situations. Member States should work to promote inclusive access for all, including local communities, to AI systems with locally relevant content and services, and with respect for multilingualism and cultural diversity. Member States should work to tackle digital divides and ensure inclusive access to and participation in the development of AI. At the national level, Member States should promote equity between rural and urban areas, and among all persons regardless of race, colour, descent, gender, age, language, religion, political opinion, national origin, ethnic origin, social origin, economic or social condition of birth, or disability and any other grounds, in terms of access to and participation in the AI system life cycle. At the international level, the most technologically advanced countries have a responsibility of solidarity with the least advanced to ensure that the benefits of AI technologies are shared such that access to and participation in the AI system life cycle for the latter contributes to a fairer world order with regard to information, communication, culture, education, research and socio-economic and political stability. 29. AI actors should make all reasonable efforts to minimize and avoid reinforcing or perpetuating discriminatory or biased applications and outcomes throughout the life cycle of the AI system to ensure fairness of such systems. Effective remedy should be available against discrimination and biased algorithmic determination. 30. Furthermore, digital and knowledge divides within and between countries need to be addressed throughout an AI system life cycle, including in terms of access and quality of access to technology and data, in accordance with relevant national, regional and international legal frameworks, as well as in terms of connectivity, knowledge and skills and meaningful participation of the affected communities, such that every person is treated equitably. Sustainability 31. The development of sustainable societies relies on the achievement of a complex set of objectives on a continuum of human, social, cultural, economic and environmental dimensions. The advent of AI technologies can either benefit sustainability objectives or hinder their realization, depending on how they are applied across countries with varying levels of development. The continuous assessment of the human, social, cultural, economic and environmental impact of AI technologies should therefore be carried out with full cognizance of the implications of AI technologies for sustainability as a set of constantly evolving goals across a range of dimensions, such as currently identified in the Sustainable Development Goals (SDGs) of the United Nations. Right to Privacy, and Data Protection 32. Privacy, a right essential to the protection of human dignity, human autonomy and human agency, must be respected, protected and promoted throughout the life cycle of AI systems. It is important that data for AI systems be collected, used, shared, archived and deleted in ways that are consistent with international law and in line with the values and principles set forth in this Recommendation, while respecting relevant national, regional and international legal frameworks. 33. Adequate data protection frameworks and governance mechanisms should be established in a multi-stakeholder approach at the national or international level, protected by judicial systems, and ensured throughout the life cycle of AI systems. Data protection frameworks and any related mechanisms should take reference from international data protection principles and standards concerning the collection, use and disclosure of personal data and exercise of their rights by data subjects while ensuring a legitimate aim and a valid legal basis for the processing of personal data, including informed consent. 34. Algorithmic systems require adequate privacy impact assessments, which also include societal and ethical considerations of their use and an innovative use of the privacy by design approach. AI actors need to ensure that they are accountable for the design and implementation of AI systems in such a way as to ensure that personal information is protected throughout the life cycle of the AI system. Human oversight and determination 35. Member States should ensure that it is always possible to attribute ethical and legal responsibility for any stage of the life cycle of AI systems, as well as in cases of remedy related to AI systems, to physical persons or to existing legal entities. Human oversight refers thus not only to individual human oversight, but to inclusive public oversight, as appropriate. 36. It may be the case that sometimes humans would choose to rely on AI systems for reasons of efficacy, but the decision to cede control in limited contexts remains that of humans, as humans can resort to AI systems in decision-making and acting, but an AI system can never replace ultimate human responsibility and accountability. As a rule, life and death decisions should not be ceded to AI systems. Transparency and explainability 37. The transparency and explainability of AI systems are often essential preconditions to ensure the respect, protection and promotion of human rights, fundamental freedoms and ethical principles. Transparency is necessary for relevant national and international liability regimes to work effectively. A lack of transparency could also undermine the possibility of effectively challenging decisions based on outcomes produced by AI systems and may thereby infringe the right to a fair trial and effective remedy, and limits the areas in which these systems can be legally used. 38. While efforts need to be made to increase transparency and explainability of AI systems, including those with extra-territorial impact, throughout their life cycle to support democratic governance, the level of transparency and explainability should always be appropriate to the context and impact, as there may be a need to balance between transparency and explainability and other principles such as privacy, safety and security. People should be fully informed when a decision is informed by or is made on the basis of AI algorithms, including when it affects their safety or human rights, and in those circumstances should have the opportunity to request explanatory information from the relevant AI actor or public sector institutions. In addition, individuals should be able to access the reasons for a decision affecting their rights and freedoms, and have the option of making submissions to a designated staff member of the private sector company or public sector institution able to review and correct the decision. AI actors should inform users when a product or service is provided directly or with the assistance of AI systems in a proper and timely manner. 39. From a socio-technical lens, greater transparency contributes to more peaceful, just, democratic and inclusive societies. It allows for public scrutiny that can decrease corruption and discrimination, and can also help detect and prevent negative impacts on human rights. Transparency aims at providing appropriate information to the respective addressees to enable their understanding and foster trust. Specific to the AI system, transparency can enable people to understand how each stage of an AI system is put in place, appropriate to the context and sensitivity of the AI system. It may also include insight into factors that affect a specific prediction or decision, and whether or not appropriate assurances (such as safety or fairness measures) are in place. In cases of serious threats of adverse human rights impacts, transparency may also require the sharing of code or datasets. 40. Explainability refers to making intelligible and providing insight into the outcome of AI systems. The explainability of AI systems also refers to the understandability of the input, output and the functioning of each algorithmic building block and how it contributes to the outcome of the systems. Thus, explainability is closely related to transparency, as outcomes and sub-processes leading to outcomes should aim to be understandable and traceable, appropriate to the context. AI actors should commit to ensuring that the algorithms developed are explainable. In the case of AI applications that impact the end user in a way that is not temporary, easily reversible or otherwise low risk, it should be ensured that the meaningful explanation is provided with any decision that resulted in the action taken in order for the outcome to be considered transparent. 41. Transparency and explainability relate closely to adequate responsibility and accountability measures, as well as to the trustworthiness of AI systems. Responsibility and accountability 42. AI actors and Member States should respect, protect and promote human rights and fundamental freedoms, and should also promote the protection of the environment and ecosystems, assuming their respective ethical and legal responsibility, in accordance with national and international law, in particular Member States' human rights obligations, and ethical guidance throughout the life cycle of AI systems, including with respect to AI actors within their effective territory and control. The ethical responsibility and liability for the decisions and actions based in any way on an AI system should always ultimately be attributable to AI actors corresponding to their role in the life cycle of the AI system. 43. Appropriate oversight, impact assessment, audit and due diligence mechanisms, including whistle-blowers' protection, should be developed to ensure accountability for AI systems and their impact throughout their life cycle. Both technical and institutional designs should ensure auditability and traceability of (the working of) AI systems in particular to address any conflicts with human rights norms and standards and threats to environmental and ecosystem well-being. Awareness and literacy 44. Public awareness and understanding of AI technologies and the value of data should be promoted through open and accessible education, civic engagement, digital skills and AI ethics training, media and information literacy and training led jointly by governments, intergovernmental organizations, civil society, academia, the media, community leaders and the private sector, and considering the existing linguistic, social and cultural diversity, to ensure effective public participation so that all members of society can take informed decisions about their use of AI systems and be protected from undue influence. 45. Learning about the impact of AI systems should include learning about, through and for human rights and fundamental freedoms, meaning that the approach and understanding of AI systems should be grounded by their impact on human rights and access to rights, as well as on the environment and ecosystems. Multi-stakeholder and adaptive governance and collaboration 46. International law and national sovereignty must be respected in the use of data. That means that States, complying with international law, can regulate the data generated within or passing through their territories, and take measures towards effective regulation of data, including data protection, based on respect for the right to privacy in accordance with international law and other human rights norms and standards. Participation of different stakeholders throughout the AI system life cycle is necessary for inclusive approaches to AI governance, enabling the benefits to be shared by all, and to contribute to sustainable development. Stakeholders include but are not limited to governments, intergovernmental organizations, the technical community, civil society, researchers and academia, media, education, policy-makers, private sector companies, human rights institutions and equality bodies, anti-discrimination monitoring bodies, and groups for youth and children. The adoption of open standards and interoperability to facilitate collaboration should be in place. Measures should be adopted to take into account shifts in technologies, the emergence of new groups of stakeholders, and to allow for meaningful participation by marginalized groups, communities and individuals and, where relevant, in the case of Indigenous Peoples, respect for the self-governance of their data. IV. AREAS OF POLICY ACTION 48. The policy actions described in the following policy areas operationalize the values and principles set out in this Recommendation. The main action is for Member States to put in place effective measures, including, for example, policy frameworks or mechanisms, and to ensure that other stakeholders, such as private sector companies, academic and research institutions, and civil society adhere to them by, among other actions, encouraging all stakeholders to develop human rights, rule of law, democracy, and ethical impact assessment and due diligence tools in line with guidance including the United Nations Guiding Principles on Business and Human Rights. The process for developing such policies or mechanisms should be inclusive of all stakeholders and should take into account the circumstances and priorities of each Member State. UNESCO can be a partner and support Member States in the development as well as monitoring and evaluation of policy mechanisms. 49. UNESCO recognizes that Member States will be at different stages of readiness to implement this Recommendation, in terms of scientific, technological, economic, educational, legal, regulatory, infrastructural, societal, cultural and other dimensions. It is noted that \"readiness\" here is a dynamic status. In order to enable the effective implementation of this Recommendation, UNESCO will therefore: (1) develop a readiness assessment methodology to assist interested Member States in identifying their status at specific moments of their readiness trajectory along a continuum of dimensions; and (2) ensure support for interested Member States in terms of developing a UNESCO methodology for Ethical Impact Assessment (EIA) of AI technologies, sharing of best practices, assessment guidelines and other mechanisms and analytical work. POLICY AREA 1: ETHICAL IMPACT ASSESSMENT 50. Member States should introduce frameworks for impact assessments, such as ethical impact assessment, to identify and assess benefits, concerns and risks of AI systems, as well as appropriate risk prevention, mitigation and monitoring measures, among other assurance mechanisms. Such impact assessments should identify impacts on human rights and fundamental freedoms, in particular but not limited to the rights of marginalized and vulnerable people or people in vulnerable situations, labour rights, the environment and ecosystems and ethical and social implications, and facilitate citizen participation in line with the values and principles set forth in this Recommendation. 51. Member States and private sector companies should develop due diligence and oversight mechanisms to identify, prevent, mitigate and account for how they address the impact of AI systems on the respect for human rights, rule of law and inclusive societies. Member States should also be able to assess the socio-economic impact of AI systems on poverty and ensure that the gap between people living in wealth and poverty, as well as the digital divide among and within countries, are not increased with the massive adoption of AI technologies at present and in the future. In order to do this, in particular, enforceable transparency protocols should be implemented, corresponding to the access to information, including information of public interest held by private entities. Member States, private sector companies and civil society should investigate the sociological and psychological effects of AI-based recommendations on humans in their decision-making autonomy. AI systems identified as potential risks to human rights should be broadly tested by AI actors, including in real-world conditions if needed, as part of the Ethical Impact Assessment, before releasing them in the market. 52. Member States and business enterprises should implement appropriate measures to monitor all phases of an AI system life cycle, including the functioning of algorithms used for decision-making, the data, as well as AI actors involved in the process, especially in public services and where direct end-user interaction is needed, as part of ethical impact assessment. Member States' human rights law obligations should form part of the ethical aspects of AI system assessments. 53. Governments should adopt a regulatory framework that sets out a procedure, particularly for public authorities, to carry out ethical impact assessments on AI systems to predict consequences, mitigate risks, avoid harmful consequences, facilitate citizen participation and address societal challenges. The assessment should also establish appropriate oversight mechanisms, including auditability, traceability and explainability, which enable the assessment of algorithms, data and design processes, as well as include external review of AI systems. Ethical impact assessments should be transparent and open to the public, where appropriate. Such assessments should also be multidisciplinary, multi-stakeholder, multicultural, pluralistic and inclusive. The public authorities should be required to monitor the AI systems implemented and/or deployed by those authorities by introducing appropriate mechanisms and tools. POLICY AREA 2: ETHICAL GOVERNANCE AND STEWARDSHIP 54. Member States should ensure that AI governance mechanisms are inclusive, transparent, multidisciplinary, multilateral (this includes the possibility of mitigation and redress of harm across borders) and multi-stakeholder. In particular, governance should include aspects of anticipation, and effective protection, monitoring of impact, enforcement and redress. 55. Member States should ensure that harms caused through AI systems are investigated and redressed, by enacting strong enforcement mechanisms and remedial actions, to make certain that human rights and fundamental freedoms and the rule of law are respected in the digital world and in the physical world. Such mechanisms and actions should include remediation mechanisms provided by private and public sector companies. The auditability and traceability of AI systems should be promoted to this end. In addition, Member States should strengthen their institutional capacities to deliver on this commitment and should collaborate with researchers and other stakeholders to investigate, prevent and mitigate any potentially malicious uses of AI systems. 56. Member States are encouraged to develop national and regional AI strategies and to consider forms of soft governance such as a certification mechanism for AI systems and the mutual recognition of their certification, according to the sensitivity of the application domain and expected impact on human rights, the environment and ecosystems, and other ethical considerations set forth in this Recommendation. Such a mechanism might include different levels of audit of systems, data, and adherence to ethical guidelines and to procedural requirements in view of ethical aspects. At the same time, such a mechanism should not hinder innovation or disadvantage small and medium enterprises or start-ups, civil society as well as research and science organizations, as a result of an excessive administrative burden. These mechanisms should also include a regular monitoring component to ensure system robustness and continued integrity and adherence to ethical guidelines over the entire life cycle of the AI system, requiring re-certification if necessary. 57. Member States and public authorities should carry out transparent self-assessment of existing and proposed AI systems, which, in particular, should include the assessment of whether the adoption of AI is appropriate and, if so, should include further assessment to determine what the appropriate method is, as well as assessment as to whether such adoption would result in violations or abuses of Member States' human rights law obligations, and if that is the case, prohibit its use. 58. Member States should encourage public entities, private sector companies and civil society organizations to involve different stakeholders in their AI governance and to consider adding the role of an independent AI Ethics Officer or some other mechanism to oversee ethical impact assessment, auditing and continuous monitoring efforts and ensure ethical guidance of AI systems. Member States, private sector companies and civil society organizations, with the support of UNESCO, are encouraged to create a network of independent AI Ethics Officers to give support to this process at national, regional and international levels. 59. Member States should foster the development of, and access to, a digital ecosystem for ethical and inclusive development of AI systems at the national level, including to address gaps in access to the AI system life cycle, while contributing to international collaboration. Such an ecosystem includes, in particular, digital technologies and infrastructure, and mechanisms for sharing AI knowledge, as appropriate. 60. Member States should establish mechanisms, in collaboration with international organizations, transnational corporations, academic institutions and civil society, to ensure the active participation of all Member States, especially LMICs, in particular LDCs, LLDCs and SIDS, in international discussions concerning AI governance. This can be through the provision of funds, ensuring equal regional participation, or any other mechanisms. Furthermore, in order to ensure the inclusiveness of AI fora, Member States should facilitate the travel of AI actors in and out of their territory, especially from LMICs, in particular LDCs, LLDCs and SIDS, for the purpose of participating in these fora. 61. Amendments to the existing or elaboration of new national legislation addressing AI systems must comply with Member States' human rights law obligations and promote human rights and fundamental freedoms throughout the AI system life cycle. Promotion thereof should also take the form of governance initiatives, good exemplars of collaborative practices regarding AI systems, and national and international technical and methodological guidelines as AI technologies advance. Diverse sectors, including the private sector, in their practices regarding AI systems must respect, protect and promote human rights and fundamental freedoms using existing and new instruments in combination with this Recommendation. 62. Member States that acquire Al systems for human rights-sensitive use cases, such as law enforcement, welfare, employment, media and information providers, health care and the independent judiciary system should provide mechanisms to monitor the social and economic impact of such systems by appropriate oversight authorities, including independent data protection authorities, sectoral oversight and public bodies responsible for oversight. 63. Member States should enhance the capacity of the judiciary to make decisions related to AI systems as per the rule of law and in line with international law and standards, including in the use of AI systems in their deliberations, while ensuring that the principle of human oversight is upheld. In case AI systems are used by the judiciary, sufficient safeguards are needed to guarantee inter alia the protection of fundamental human rights, the rule of law, judicial independence as well as the principle of human oversight, and to ensure a trustworthy, public interest-oriented and human-centric development and use of AI systems in the judiciary. 64. Member States should ensure that governments and multilateral organizations play a leading role in ensuring the safety and security of AI systems, with multi-stakeholder participation. Specifically, Member States, international organizations and other relevant bodies should develop international standards that describe measurable, testable levels of safety and transparency, so that systems can be objectively assessed and levels of compliance determined. Furthermore, Member States and business enterprises should continuously support strategic research on potential safety and security risks of AI technologies and should encourage research into transparency and explainability, inclusion and literacy by putting additional funding into those areas for different domains and at different levels, such as technical and natural language. 65. Member States should implement policies to ensure that the actions of AI actors are consistent with international human rights law, standards and principles throughout the life cycle of AI systems, while taking into full consideration the current cultural and social diversities, including local customs and religious traditions, with due regard to the precedence and universality of human rights. 66. Member States should put in place mechanisms to require AI actors to disclose and combat any kind of stereotyping in the outcomes of AI systems and data, whether by design or by negligence, and to ensure that training data sets for AI systems do not foster cultural, economic or social inequalities, prejudice, the spreading of disinformation and misinformation, and disruption of freedom of expression and access to information. Particular attention should be given to regions where the data are scarce. 67. Member States should implement policies to promote and increase diversity and inclusiveness that reflect their populations in AI development teams and training datasets, and to ensure equal access to AI technologies and their benefits, particularly for marginalized groups, both from rural and urban zones. 68. Member States should develop, review and adapt, as appropriate, regulatory frameworks to achieve accountability and responsibility for the content and outcomes of AI systems at the different phases of their life cycle. Member States should, where necessary, introduce liability frameworks or clarify the interpretation of existing frameworks to ensure the attribution of accountability for the outcomes and the functioning of AI systems. Furthermore, when developing regulatory frameworks, Member States should, in particular, take into account that ultimate responsibility and accountability must always lie with natural or legal persons and that AI systems should not be given legal personality themselves. To ensure this, such regulatory frameworks should be consistent with the principle of human oversight and establish a comprehensive approach focused on AI actors and the technological processes involved across the different stages of the AI system life cycle. 69. In order to establish norms where these do not exist, or to adapt the existing legal frameworks, Member States should involve all AI actors (including, but not limited to, researchers, representatives of civil society and law enforcement, insurers, investors, manufacturers, engineers, lawyers and users). The norms can mature into best practices, laws and regulations. Member States are further encouraged to use mechanisms such as policy prototypes and regulatory sandboxes to accelerate the development of laws, regulations and policies, including regular reviews thereof, in line with the rapid development of new technologies and ensure that laws and regulations can be tested in a safe environment before being officially adopted. Member States should support local governments in the development of local policies, regulations and laws in line with national and international legal frameworks. 70. Member States should set clear requirements for AI system transparency and explainability so as to help ensure the trustworthiness of the full AI system life cycle. Such requirements should involve the design and implementation of impact mechanisms that take into consideration the nature of application domain, intended use, target audience and feasibility of each particular AI system. POLICY AREA 3: DATA POLICY 71. Member States should work to develop data governance strategies that ensure the continual evaluation of the quality of training data for AI systems including the adequacy of the data collection and selection processes, proper data security and protection measures, as well as feedback mechanisms to learn from mistakes and share best practices among all AI actors. 72. Member States should put in place appropriate safeguards to protect the right to privacy in accordance with international law, including addressing concerns such as surveillance. Member States should, among others, adopt or enforce legislative frameworks that provide appropriate protection, compliant with international law. Member States should strongly encourage all AI actors, including business enterprises, to follow existing international standards and, in particular, to carry out adequate privacy impact assessments, as part of ethical impact assessments, which take into account the wider socio-economic impact of the intended data processing, and to apply privacy by design in their systems. Privacy should be respected, protected and promoted throughout the life cycle of AI systems. 73. Member States should ensure that individuals retain rights over their personal data and are protected by a framework, which notably foresees: transparency; appropriate safeguards for the processing of sensitive data; an appropriate level of data protection; effective and meaningful accountability schemes and mechanisms; the full enjoyment of the data subjects' rights and the ability to access and erase their personal data in AI systems, except for certain circumstances in compliance with international law; an appropriate level of protection in full compliance with data protection legislation where data are being used for commercial purposes such as enabling micro-targeted advertising, transferred cross-border; and an effective independent oversight as part of a data governance mechanism which keeps individuals in control of their personal data and fosters the benefits of a free flow of information internationally, including access to data. 74. Member States should establish their data policies or equivalent frameworks, or reinforce existing ones, to ensure full security for personal data and sensitive data, which, if disclosed, may cause exceptional damage, injury or hardship to individuals. Examples include data relating to offences, criminal proceedings and convictions, and related security measures; biometric, genetic and health data; and personal data such as that relating to race, colour, descent, gender, age, language, religion, political opinion, national origin, ethnic origin, social origin, economic or social condition of birth, or disability and any other characteristics. 75. Member States should promote open data. In this regard, Member States should consider reviewing their policies and regulatory frameworks, including on access to information and open government to reflect AI-specific requirements and promoting mechanisms, such as open repositories for publicly funded or publicly held data and source code and data trusts, to support the safe, fair, legal and ethical sharing of data, among others. 76. Member States should promote and facilitate the use of quality and robust datasets for training, development and use of AI systems, and exercise vigilance in overseeing their collection and use. This could, if possible and feasible, include investing in the creation of gold standard datasets, including open and trustworthy datasets, which are diverse, constructed on a valid legal basis, including consent of data subjects, when required by law. Standards for annotating datasets should be encouraged, including disaggregating data on gender and other bases, so it can easily be determined how a dataset is gathered and what properties it has. 77. Member States, as also suggested in the report of the United Nations Secretary-General's High-level Panel on Digital Cooperation, with the support of the United Nations and UNESCO, should adopt a digital commons approach to data where appropriate, increase interoperability of tools and datasets and interfaces of systems hosting data, and encourage private sector companies to share the data they collect with all stakeholders, as appropriate, for research, innovation or public benefits. They should also promote public and private efforts to create collaborative platforms to share quality data in trusted and secured data spaces. POLICY AREA 4: DEVELOPMENT AND INTERNATIONAL COOPERATION 78. Member States and transnational corporations should prioritize AI ethics by including discussions of AI-related ethical issues into relevant international, intergovernmental and multi-stakeholder fora. 79. Member States should ensure that the use of AI in areas of development such as education, science, culture, communication and information, health care, agriculture and food supply, environment, natural resource and infrastructure management, economic planning and growth, among others, adheres to the values and principles set forth in this Recommendation. 80. Member States should work through international organizations to provide platforms for international cooperation on AI for development, including by contributing expertise, funding, data, domain knowledge, infrastructure, and facilitating multi-stakeholder collaboration to tackle challenging development problems, especially for LMICs, in particular LDCs, LLDCs and SIDS. 81. Member States should work to promote international collaboration on AI research and innovation, including research and innovation centres and networks that promote greater participation and leadership of researchers from LMICs and other countries, including LDCs, LLDCs and SIDS. 82. Member States should promote AI ethics research by engaging international organizations and research institutions, as well as transnational corporations, that can be a basis for the ethical use of AI systems by public and private entities, including research into the applicability of specific ethical frameworks in specific cultures and contexts, and the possibilities to develop technologically feasible solutions in line with these frameworks. 83. Member States should encourage international cooperation and collaboration in the field of AI to bridge geo-technological lines. Technological exchanges and consultations should take place between Member States and their populations, between the public and private sectors, and between and among the most and least technologically advanced countries in full respect of international law. POLICY AREA 5: ENVIRONMENT AND ECOSYSTEMS 84. Member States and business enterprises should assess the direct and indirect environmental impact throughout the AI system life cycle, including, but not limited to, its carbon footprint, energy consumption and the environmental impact of raw material extraction for supporting the manufacturing of AI technologies, and reduce the environmental impact of AI systems and data infrastructures. Member States should ensure compliance of all AI actors with environmental law, policies and practices. 85. Member States should introduce incentives, when needed and appropriate, to ensure the development and adoption of rights-based and ethical AI-powered solutions for disaster risk resilience; the monitoring, protection and regeneration of the environment and ecosystems; and the preservation of the planet. These AI systems should involve the participation of local and indigenous communities throughout the life cycle of AI systems and should support circular economy type approaches and sustainable consumption and production patterns. Some examples include using AI systems, when needed and appropriate, to: (a) Support the protection, monitoring and management of natural resources. (b) Support the prediction, prevention, control and mitigation of climate-related problems. (c) Support a more efficient and sustainable food ecosystem. (d) Support the acceleration of access to and mass adoption of sustainable energy. (e) Enable and promote the mainstreaming of sustainable infrastructure, sustainable business models and sustainable finance for sustainable development. (f) Detect pollutants or predict levels of pollution and thus help relevant stakeholders identify, plan and put in place targeted interventions to prevent and reduce pollution and exposure. 86. When choosing AI methods, given the potential data-intensive or resource-intensive character of some of them and the respective impact on the environment, Member States should ensure that AI actors, in line with the principle of proportionality, favour data, energy and resource-efficient AI methods. Requirements should be developed to ensure that appropriate evidence is available to show that an AI application will have the intended effect, or that safeguards accompanying an AI application can support the justification for its use. If this cannot be done, the precautionary principle must be favoured, and in instances where there are disproportionate negative impacts on the environment, AI should not be used. POLICY AREA 6: GENDER 87. Member States should ensure that the potential for digital technologies and artificial intelligence to contribute to achieving gender equality is fully maximized, and must ensure that the human rights and fundamental freedoms of girls and women, and their safety and integrity are not violated at any stage of the AI system life cycle. Moreover, Ethical Impact Assessment should include a transversal gender perspective. 88. Member States should have dedicated funds from their public budgets linked to financing gender-responsive schemes, ensure that national digital policies include a gender action plan, and develop relevant policies, for example, on labour education, targeted at supporting girls and women to make sure they are not left out of the digital economy powered by AI. Special investment in providing targeted programmes and gender-specific language, to increase the opportunities of girls' and women's participation in science, technology, engineering, and mathematics (STEM), including information and communication technologies (ICT) disciplines, preparedness, employability, equal career development and professional growth of girls and women, should be considered and implemented. 89. Member States should ensure that the potential of AI systems to advance the achievement of gender equality is realized. They should ensure that these technologies do not exacerbate the already wide gender gaps existing in several fields in the analogue world, and instead eliminate those gaps. These gaps include: the gender wage gap; the unequal representation in certain professions and activities; the lack of representation at top management positions, boards of directors, or research teams in the AI field; the education gap; the digital and AI access, adoption, usage and affordability gap; and the unequal distribution of unpaid work and of the caring responsibilities in our societies. 90. Member States should ensure that gender stereotyping and discriminatory biases are not translated into AI systems, and instead identify and proactively redress these. Efforts are necessary to avoid the compounding negative effect of technological divides in achieving gender equality and avoiding violence such as harassment, bullying or trafficking of girls and women and under-represented groups, including in the online domain. 91. Member States should encourage female entrepreneurship, participation and engagement in all stages of an AI system life cycle by offering and promoting economic, regulatory incentives, among other incentives and support schemes, as well as policies that aim at a balanced gender participation in AI research in academia, gender representation on digital and AI companies' top management positions, boards of directors and research teams. Member States should ensure that public funds (for innovation, research and technologies) are channelled to inclusive programmes and companies, with clear gender representation, and that private funds are similarly encouraged through affirmative action principles. Policies on harassment-free environments should be developed and enforced, together with the encouragement of the transfer of best practices on how to promote diversity throughout the AI system life cycle. 92. Member States should promote gender diversity in AI research in academia and industry by offering incentives to girls and women to enter the field, putting in place mechanisms to fight gender stereotyping and harassment within the AI research community, and encouraging academic and private entities to share best practices on how to enhance gender diversity. 93. UNESCO can help form a repository of best practices for incentivizing the participation of girls, women and under-represented groups in all stages of the AI system life cycle. POLICY AREA 7: CULTURE 94. Member States are encouraged to incorporate AI systems, where appropriate, in the preservation, enrichment, understanding, promotion, management and accessibility of tangible, documentary and intangible cultural heritage, including endangered languages as well as indigenous languages and knowledges, for example by introducing or updating educational programmes related to the application of AI systems in these areas, where appropriate, and by ensuring a participatory approach, targeted at institutions and the public. 95. Member States are encouraged to examine and address the cultural impact of AI systems, especially natural language processing (NLP) applications such as automated translation and voice assistants, on the nuances of human language and expression. Such assessments should provide input for the design and implementation of strategies that maximize the benefits from these systems by bridging cultural gaps and increasing human understanding, as well as addressing the negative implications such as the reduction of use, which could lead to the disappearance of endangered languages, local dialects, and tonal and cultural variations associated with human language and expression. 96. Member States should promote AI education and digital training for artists and creative professionals to assess the suitability of AI technologies for use in their profession, and contribute to the design and implementation of suitable AI technologies, as AI technologies are being used to create, produce, distribute, broadcast and consume a variety of cultural goods and services, bearing in mind the importance of preserving cultural heritage, diversity and artistic freedom. 97. Member States should promote awareness and evaluation of AI tools among local cultural industries and small and medium enterprises working in the field of culture, to avoid the risk of concentration in the cultural market. 98. Member States should engage technology companies and other stakeholders to promote a diverse supply of and plural access to cultural expressions, and in particular to ensure that algorithmic recommendation enhances the visibility and discoverability of local content. 99. Member States should foster new research at the intersection between AI and intellectual property (IP), for example to determine whether or how to protect with IP rights the works created by means of Al technologies. Member States should also assess how AI technologies are affecting the rights or interests of IP owners, whose works are used to research, develop, train or implement AI applications. 100. Member States should encourage museums, galleries, libraries and archives at the national level to use AI systems to highlight their collections and enhance their libraries, databases and knowledge base, while also providing access to their users. POLICY AREA 8: EDUCATION AND RESEARCH 101. Member States should work with international organizations, educational institutions and private and non-governmental entities to provide adequate AI literacy education to the public on all levels in all countries in order to empower people and reduce the digital divides and digital access inequalities resulting from the wide adoption of AI systems. 102. Member States should promote the acquisition of 'prerequisite skills' for AI education, such as basic literacy, numeracy, coding and digital skills, and media and information literacy, as well as critical and creative thinking, teamwork, communication, socio-emotional and AI ethics skills, especially in countries and in regions or areas within countries where there are notable gaps in the education of these skills. 103. Member States should promote general awareness programmes about AI developments, including on data and the opportunities and challenges brought about by AI technologies, the impact of AI systems on human rights and their implications, including children's rights. These programmes should be accessible to non-technical as well as technical groups. 104. Member States should encourage research initiatives on the responsible and ethical use of AI technologies in teaching, teacher training and e-learning, among other issues, to enhance opportunities and mitigate the challenges and risks involved in this area. The initiatives should be accompanied by an adequate assessment of the quality of education and impact on students and teachers of the use of AI technologies. Member States should also ensure that AI technologies empower students and teachers and enhance their experience, bearing in mind that relational and social aspects and the value of traditional forms of education are vital in teacher-student and student-student relationships and should be considered when discussing the adoption of AI technologies in education. AI systems used in learning should be subject to strict requirements when it comes to the monitoring, assessment of abilities, or prediction of the learners' behaviours. AI should support the learning process without reducing cognitive abilities and without extracting sensitive information, in compliance with relevant personal data protection standards. The data handed over to acquire knowledge collected during the learner's interactions with the AI system must not be subject to misuse, misappropriation or criminal exploitation, including for commercial purposes. 105. Member States should promote the participation and leadership of girls and women, diverse ethnicities and cultures, persons with disabilities, marginalized and vulnerable people or people in vulnerable situations, minorities and all persons not enjoying the full benefits of digital inclusion, in AI education programmes at all levels, as well as the monitoring and sharing of best practices in this regard with other Member States. 106. Member States should develop, in accordance with their national education programmes and traditions, AI ethics curricula for all levels, and promote cross-collaboration between AI technical skills education and humanistic, ethical and social aspects of AI education. Online courses and digital resources of AI ethics education should be developed in local languages, including indigenous languages, and take into account the diversity of environments, especially ensuring accessibility of formats for persons with disabilities. 107. Member States should promote and support AI research, notably AI ethics research, including for example through investing in such research or by creating incentives for the public and private sectors to invest in this area, recognizing that research contributes significantly to the further development and improvement of AI technologies with a view to promoting international law and the values and principles set forth in this Recommendation. Member States should also publicly promote the best practices of, and cooperation with, researchers and companies who develop AI in an ethical manner. 108. Member States should ensure that AI researchers are trained in research ethics and require them to include ethical considerations in their designs, products and publications, especially in the analyses of the datasets they use, how they are annotated, and the quality and scope of the results with possible applications. Member States should encourage private sector companies to facilitate the access of the scientific community to their data for research, especially in LMICs, in particular LDCs, LLDCs and SIDS. This access should conform to relevant privacy and data protection standards. 110. To ensure a critical evaluation of AI research and proper monitoring of potential misuses or adverse effects, Member States should ensure that any future developments with regards to AI technologies should be based on rigorous and independent scientific research, and promote interdisciplinary AI research by including disciplines other than science, technology, engineering and mathematics (STEM), such as cultural studies, education, ethics, international relations, law, linguistics, philosophy, political science, sociology and psychology. 111. Recognizing that AI technologies present great opportunities to help advance scientific knowledge and practice, especially in traditionally model-driven disciplines, Member States should encourage scientific communities to be aware of the benefits, limits and risks of their use; this includes attempting to ensure that conclusions drawn from data-driven approaches, models and treatments are robust and sound. Furthermore, Member States should welcome and support the role of the scientific community in contributing to policy and in cultivating awareness of the strengths and weaknesses of AI technologies. POLICY AREA 9: COMMUNICATION AND INFORMATION 112. Member States should use AI systems to improve access to information and knowledge. This can include support to researchers, academia, journalists, the general public and developers, to enhance freedom of expression, academic and scientific freedoms, access to information, and increased proactive disclosure of official data and information. 113. Member States should ensure that AI actors respect and promote freedom of expression as well as access to information with regard to automated content generation, moderation and curation. Appropriate frameworks, including regulation, should enable transparency of online communication and information operators and ensure users have access to a diversity of viewpoints, as well as processes for prompt notification to the users on the reasons for removal or other treatment of content, and appeal mechanisms that allow users to seek redress. 114. Member States should invest in and promote digital and media and information literacy skills to strengthen critical thinking and competencies needed to understand the use and implication of AI systems, in order to mitigate and counter disinformation, misinformation and hate speech. A better understanding and evaluation of both the positive and potentially harmful effects of recommender systems should be part of those efforts. 115. Member States should create enabling environments for media to have the rights and resources to effectively report on the benefits and harms of AI systems, and also encourage media to make ethical use of AI systems in their operations. POLICY AREA 10: ECONOMY AND LABOUR 116. Member States should assess and address the impact of AI systems on labour markets and its implications for education requirements, in all countries and with special emphasis on countries where the economy is labour-intensive. This can include the introduction of a wider range of 'core' and interdisciplinary skills at all education levels to provide current workers and new generations a fair chance of finding jobs in a rapidly changing market, and to ensure their awareness of the ethical aspects of AI systems. Skills such as 'learning how to learn', communication, critical thinking, teamwork, empathy, and the ability to transfer one's knowledge across domains, should be taught alongside specialist, technical skills, as well as low-skilled tasks. Being transparent about what skills are in demand and updating curricula around these are key. 117. Member States should support collaboration agreements among governments, academic institutions, vocational education and training institutions, industry, workers' organizations and civil society to bridge the gap of skillset requirements to align training programmes and strategies with the implications of the future of work and the needs of industry, including small and medium enterprises. Project-based teaching and learning approaches for AI should be promoted, allowing for partnerships between public institutions, private sector companies, universities and research centres. 118. Member States should work with private sector companies, civil society organizations and other stakeholders, including workers and unions to ensure a fair transition for at-risk employees. This includes putting in place upskilling and reskilling programmes, finding effective mechanisms of retaining employees during those transition periods, and exploring 'safety net' programmes for those who cannot be retrained. Member States should develop and implement programmes to research and address the challenges identified that could include upskilling and reskilling, enhanced social protection, proactive industry policies and interventions, tax benefits, new taxation forms, among others. Member States should ensure that there is sufficient public funding to support these programmes. Relevant regulations, such as tax regimes, should be carefully examined and changed if needed to counteract the consequences of unemployment caused by AI-based automation. 119. Member States should encourage and support researchers to analyze the impact of AI systems on the local labour environment in order to anticipate future trends and challenges. These studies should have an interdisciplinary approach and investigate the impact of AI systems on economic, social and geographic sectors, as well as on human-robot interactions and human-human relationships, in order to advise on reskilling and redeployment best practices. 120. Member States should take appropriate steps to ensure competitive markets and consumer protection, considering possible measures and mechanisms at national, regional and international levels, to prevent abuse of dominant market positions, including by monopolies, in relation to AI systems throughout their life cycle, whether these are data, research, technology, or market. Member States should prevent the resulting inequalities, assess relevant markets and promote competitive markets. Due consideration should be given to LMICs, in particular LDCs, LLDCs and SIDS, which are more exposed and vulnerable to the possibility of abuses of market dominance as a result of a lack of infrastructure, human capacity and regulations, among other factors. AI actors developing AI systems in countries which have established or adopted ethical standards on AI should respect these standards when exporting these products, developing or applying their AI systems in countries where such standards may not exist, while respecting applicable international law and domestic legislation, standards and practices of these countries. POLICY AREA 11: HEALTH AND SOCIAL WELL-BEING 121. Member States should endeavour to employ effective AI systems for improving human health and protecting the right to life, including mitigating disease outbreaks, while building and maintaining international solidarity to tackle global health risks and uncertainties, and ensure that their deployment of AI systems in health care be consistent with international law and their human rights law obligations. Member States should ensure that actors involved in health care AI systems take into consideration the importance of a patient's relationships with their family and with health care staff. 122. Member States should ensure that the development and deployment of AI systems related to health in general and mental health in particular, paying due attention to children and youth, is regulated to the effect that they are safe, effective, efficient, scientifically and medically proven and enable evidence-based innovation and medical progress. Moreover, in the related area of digital health interventions, Member States are strongly encouraged to actively involve patients and their representatives in all relevant steps of the development of the system. (a) ensuring oversight to minimize and mitigate bias; (b) ensuring that the professional, the patient, caregiver or service user is included as a 'domain expert' in the team in all relevant steps when developing the algorithms; (c) paying due attention to privacy because of the potential need for being medically monitored and ensuring that all relevant national and international data protection requirements are met; (d) ensuring effective mechanisms so that those whose personal data is being analysed are aware of and provide informed consent for the use and analysis of their data, without preventing access to health care; (e) ensuring the human care and final decision of diagnosis and treatment are taken always by humans while acknowledging that AI systems can also assist in their work; (f) ensuring, where necessary, the review of AI systems by an ethical research committee prior to clinical use. 124. Member States should establish research on the effects and regulation of potential harms to mental health related to AI systems, such as higher degrees of depression, anxiety, social isolation, developing addiction, trafficking, radicalization and misinformation, among others. 125. Member States should develop guidelines for human-robot interactions and their impact on human-human relationships, based on research and directed at the future development of robots, and with special attention to the mental and physical health of human beings. Particular attention should be given to the use of robots in health care and the care for older persons and persons with disabilities, in education, and robots for use by children, toy robots, chatbots and companion robots for children and adults. Furthermore, assistance of AI technologies should be applied to increase the safety and ergonomic use of robots, including in a human-robot working environment. Special attention should be paid to the possibility of using AI to manipulate and abuse human cognitive biases. 126. Member States should ensure that human-robot interactions comply with the same values and principles that apply to any other AI systems, including human rights and fundamental freedoms, the promotion of diversity, and the protection of vulnerable people or people in vulnerable situations. Ethical questions related to AI-powered systems for neurotechnologies and brain-computer interfaces should be considered in order to preserve human dignity and autonomy. 127. Member States should ensure that users can easily identify whether they are interacting with a living being, or with an AI system imitating human or animal characteristics, and can effectively refuse such interaction and request human intervention. 128. Member States should implement policies to raise awareness about the anthropomorphization of AI technologies and technologies that recognize and mimic human emotions, including in the language used to mention them, and assess the manifestations, ethical implications and possible limitations of such anthropomorphization, in particular in the context of robot-human interaction and especially when children are involved. 129. Member States should encourage and promote collaborative research into the effects of long-term interaction of people with AI systems, paying particular attention to the psychological and cognitive impact that these systems can have on children and young people. This should be done using multiple norms, principles, protocols, disciplinary approaches, and assessment of the modification of behaviours and habits, as well as careful evaluation of the downstream cultural and societal impacts. Furthermore, Member States should encourage research on the effect of AI technologies on health system performance and health outcomes. 130. Member States, as well as all stakeholders, should put in place mechanisms to meaningfully engage children and young people in conversations, debates and decision-making with regard to the impact of AI systems on their lives and futures. V. MONITORING AND EVALUATION Member States should, according to their specific conditions, governing structures and constitutional provisions, credibly and transparently monitor and evaluate policies, programmes and mechanisms related to ethics of AI, using a combination of quantitative and qualitative approaches. To support Member States, UNESCO can contribute by: (a) developing a UNESCO methodology for Ethical Impact Assessment (EIA) of AI technologies based on rigorous scientific research and grounded in international human rights law, guidance for its implementation in all stages of the AI system life cycle, and capacity-building materials to support Member States' efforts to train government officials, policy-makers and other relevant AI actors on EIA methodology; (b) developing a UNESCO readiness assessment methodology to assist Member States in identifying their status at specific moments of their readiness trajectory along a continuum of dimensions; (c) developing a UNESCO methodology to evaluate ex ante and ex post the effectiveness and efficiency of the policies for AI ethics and incentives against defined objectives; (d) strengthening the research- and evidence-based analysis of and reporting on policies regarding AI ethics; (e) collecting and disseminating progress, innovations, research reports, scientific publications, data and statistics regarding policies for AI ethics, including through existing initiatives, to support sharing best practices and mutual learning, and to advance the implementation of this Recommendation. 132. Processes for monitoring and evaluation should ensure broad participation of all stakeholders, including, but not limited to, vulnerable people or people in vulnerable situations. Social, cultural and gender diversity should be ensured, with a view to improving learning processes and strengthening the connections between findings, decision-making, transparency and accountability for results. 133. In the interests of promoting best policies and practices related to ethics of AI, appropriate tools and indicators should be developed for assessing the effectiveness and efficiency thereof against agreed standards, priorities and targets, including specific targets for persons belonging to disadvantaged, marginalized populations, and vulnerable people or people in vulnerable situations, as well as the impact of AI systems at individual and societal levels. The monitoring and assessment of the impact of AI systems and related AI ethics policies and practices should be carried out continuously in a systematic way proportionate to the relevant risks. This should be based on internationally agreed frameworks and involve evaluations of private and public institutions, providers and programmes, including self-evaluations, as well as tracer studies and the development of sets of indicators. Data collection and processing should be conducted in accordance with international law, national legislation on data protection and data privacy, and the values and principles outlined in this Recommendation. 134. In particular, Member States may wish to consider possible mechanisms for monitoring and evaluation, such as an ethics commission, AI ethics observatory, repository covering human rights-compliant and ethical development of AI systems, or contributions to existing initiatives by addressing adherence to ethical principles across UNESCO's areas of competence, an experience-sharing mechanism, AI regulatory sandboxes, and an assessment guide for all AI actors to evaluate their adherence to policy recommendations mentioned in this document. VI. UTILIZATION AND EXPLOITATION OF THE PRESENT RECOMMENDATION 135. Member States and all other stakeholders as identified in this Recommendation should respect, promote and protect the ethical values, principles and standards regarding AI that are identified in this Recommendation, and should take all feasible steps to give effect to its policy recommendations. 136. Member States should strive to extend and complement their own action in respect of this Recommendation, by cooperating with all relevant national and international governmental and non-governmental organizations, as well as transnational corporations and scientific organizations, whose activities fall within the scope and objectives of this Recommendation. The development of a UNESCO Ethical Impact Assessment methodology and the establishment of national commissions for the ethics of AI can be important instruments for this. VII. PROMOTION OF THE PRESENT RECOMMENDATION 137. UNESCO has the vocation to be the principal United Nations agency to promote and disseminate this Recommendation, and accordingly will work in collaboration with other relevant United Nations entities, while respecting their mandate and avoiding duplication of work. 138. UNESCO, including its bodies, such as the World Commission on the Ethics of Scientific Knowledge and Technology (COMEST), the International Bioethics Committee (IBC) and the Intergovernmental Bioethics Committee (IGBC), will also work in collaboration with other international, regional and sub-regional governmental and non-governmental organizations. 139. Even though, within UNESCO, the mandate to promote and protect falls within the authority of governments and intergovernmental bodies, civil society will be an important actor to advocate for the public sector's interests and therefore UNESCO needs to ensure and promote its legitimacy. VIII. FINAL PROVISIONS 140. This Recommendation needs to be understood as a whole, and the foundational values and principles are to be understood as complementary and interrelated. 141. Nothing in this Recommendation may be interpreted as replacing, altering or otherwise prejudicing States' obligations or rights under international law, or as approval for any State, other political, economic or social actor, group or person to engage in any activity or perform any act contrary to human rights, fundamental freedoms, human dignity and concern for the environment and ecosystems, both living and non-living.",
  "embedding": [
    0.1748046875,
    -0.1640625,
    0.1416015625,
    0.007171630859375,
    0.0284423828125,
    -0.09375,
    0.01361083984375,
    0.021728515625,
    -0.0062255859375,
    0.01080322265625,
    0.03271484375,
    0.10546875,
    0.032470703125,
    -0.0380859375,
    0.11474609375,
    -0.041748046875,
    -0.06396484375,
    0.03662109375,
    -0.031982421875,
    -0.11376953125,
    0.031494140625,
    -0.10205078125,
    0.08642578125,
    0.1396484375,
    -0.0947265625,
    0.060546875,
    -0.0274658203125,
    0.0390625,
    0.003936767578125,
    -0.0020599365234375,
    0.042236328125,
    -0.08984375,
    0.0306396484375,
    -0.07275390625,
    0.014404296875,
    0.0162353515625,
    0.0751953125,
    -0.015869140625,
    0.00701904296875,
    -0.03466796875,
    -0.0101318359375,
    -0.044921875,
    0.04931640625,
    -0.06787109375,
    0.050048828125,
    0.07666015625,
    0.06494140625,
    0.00848388671875,
    -0.0693359375,
    -0.0322265625,
    0.1005859375,
    0.08203125,
    -0.08251953125,
    0.07080078125,
    0.0016021728515625,
    0.037109375,
    0.04150390625,
    -0.011474609375,
    -0.06982421875,
    -0.0091552734375,
    -0.06640625,
    0.060546875,
    -0.044921875,
    -0.013427734375,
    -0.016845703125,
    0.09326171875,
    -0.0235595703125,
    -0.05615234375,
    -0.00848388671875,
    -0.00830078125,
    -0.036376953125,
    0.0181884765625,
    -0.043701171875,
    -0.0040283203125,
    0.012451171875,
    0.0203857421875,
    -0.0281982421875,
    0.06591796875,
    -0.019775390625,
    0.0223388671875,
    0.08837890625,
    0.04345703125,
    0.03369140625,
    -0.0027313232421875,
    0.028564453125,
    0.05029296875,
    -0.072265625,
    0.002716064453125,
    0.0269775390625,
    -0.028564453125,
    0.0291748046875,
    -0.0634765625,
    -0.053466796875,
    -0.03759765625,
    0.08984375,
    0.01092529296875,
    -0.0257568359375,
    0.01806640625,
    -0.037841796875,
    -0.01019287109375,
    -0.0732421875,
    0.0311279296875,
    -0.080078125,
    0.039306640625,
    -0.005157470703125,
    0.052734375,
    -0.0096435546875,
    -0.01324462890625,
    -0.076171875,
    -0.02783203125,
    -0.0062255859375,
    -0.00738525390625,
    0.00830078125,
    0.00732421875,
    0.00787353515625,
    0.048583984375,
    -0.036865234375,
    0.06591796875,
    0.05615234375,
    -0.026611328125,
    0.01556396484375,
    -0.041015625,
    -0.017822265625,
    0.01177978515625,
    0.0159912109375,
    -0.0400390625,
    -0.039306640625,
    0.04931640625,
    0.010009765625,
    -0.0703125,
    0.00567626953125,
    0.04541015625,
    -0.052490234375,
    -0.002838134765625,
    -0.0029449462890625,
    0.0206298828125,
    -0.0021820068359375,
    -0.029541015625,
    0.00189208984375,
    0.0152587890625,
    0.006103515625,
    -0.00018405914306640625,
    -0.051025390625,
    0.0133056640625,
    0.01141357421875,
    -0.016357421875,
    -0.02978515625,
    -0.00194549560546875,
    -0.007568359375,
    0.054443359375,
    0.052978515625,
    0.012451171875,
    -0.012939453125,
    -0.0205078125,
    0.044677734375,
    0.0177001953125,
    0.044677734375,
    0.0108642578125,
    0.0029449462890625,
    0.00982666015625,
    -0.0184326171875,
    0.0269775390625,
    -0.00421142578125,
    -0.037841796875,
    -0.039794921875,
    0.0185546875,
    -0.0167236328125,
    0.021484375,
    -0.0162353515625,
    -0.0038604736328125,
    -0.044921875,
    0.043701171875,
    0.02783203125,
    -0.0242919921875,
    -0.049560546875,
    0.004150390625,
    0.00885009765625,
    0.08349609375,
    0.03271484375,
    0.056640625,
    0.0205078125,
    -0.0218505859375,
    -0.029541015625,
    0.04248046875,
    -0.08349609375,
    0.00225830078125,
    0.0152587890625,
    -0.0107421875,
    0.006622314453125,
    -0.007659912109375,
    -0.006011962890625,
    0.0033111572265625,
    0.053466796875,
    0.0537109375,
    -0.0289306640625,
    0.0306396484375,
    -0.038818359375,
    0.01031494140625,
    -0.015869140625,
    -0.04150390625,
    0.007720947265625,
    -0.01513671875,
    0.007171630859375,
    0.02880859375,
    -0.024658203125,
    0.01611328125,
    -0.00537109375,
    0.0146484375,
    -0.0162353515625,
    0.0267333984375,
    -0.00057220458984375,
    0.023193359375,
    0.0091552734375,
    -0.01104736328125,
    -0.06982421875,
    -0.025146484375,
    0.055419921875,
    0.053466796875,
    0.00897216796875,
    -0.00015735626220703125,
    0.0091552734375,
    -0.0186767578125,
    0.0185546875,
    -0.01171875,
    -0.0177001953125,
    0.004547119140625,
    -0.017578125,
    0.033935546875,
    -0.0245361328125,
    0.040771484375,
    0.022705078125,
    0.01556396484375,
    -0.0263671875,
    0.0224609375,
    0.048583984375,
    -0.0308837890625,
    -0.0189208984375,
    0.056640625,
    0.08251953125,
    -0.003265380859375,
    -0.05224609375,
    -0.0284423828125,
    -0.0220947265625,
    0.01348876953125,
    -0.052978515625,
    -0.0030364990234375,
    -0.04736328125,
    -0.0238037109375,
    -0.041015625,
    -0.005401611328125,
    -0.00213623046875,
    -0.006744384765625,
    -0.042724609375,
    -0.03466796875,
    0.02587890625,
    -0.03369140625,
    -0.00701904296875,
    -0.01397705078125,
    -0.02685546875,
    0.0203857421875,
    -0.02587890625,
    -0.0252685546875,
    -0.026123046875,
    -0.01318359375,
    0.0260009765625,
    -0.019287109375,
    -0.00994873046875,
    -0.01220703125,
    0.00982666015625,
    -0.0283203125,
    -0.00145721435546875,
    -0.01513671875,
    -0.010009765625,
    0.06298828125,
    0.01324462890625,
    0.0400390625,
    -0.02099609375,
    0.021484375,
    0.025146484375,
    -0.029052734375,
    -0.05029296875,
    -0.0157470703125,
    0.0286865234375,
    0.006439208984375,
    -0.0302734375,
    -0.0279541015625,
    -0.0047607421875,
    0.031982421875,
    0.000732421875,
    -0.0007476806640625,
    0.01171875,
    -0.0142822265625,
    0.0027618408203125,
    0.02294921875,
    0.003509521484375,
    -0.0004787445068359375,
    -0.07763671875,
    0.005889892578125,
    0.042236328125,
    0.007781982421875,
    -0.019775390625,
    0.010986328125,
    0.044189453125,
    0.041259765625,
    -0.005462646484375,
    -0.00579833984375,
    0.0147705078125,
    -0.044189453125,
    -0.0233154296875,
    0.00604248046875,
    -0.0159912109375,
    -0.0235595703125,
    0.02587890625,
    0.0283203125,
    -0.0498046875,
    -0.0032958984375,
    -0.0302734375,
    0.01544189453125,
    -0.01116943359375,
    -0.0025482177734375,
    -0.017822265625,
    0.0040283203125,
    -0.0400390625,
    0.032470703125,
    -0.041015625,
    -0.0322265625,
    0.003814697265625,
    -0.034912109375,
    0.0137939453125,
    0.00089263916015625,
    -0.0031585693359375,
    -0.0081787109375,
    0.0091552734375,
    0.06494140625,
    0.00421142578125,
    -0.0257568359375,
    -0.040771484375,
    0.0106201171875,
    0.009765625,
    0.00102996826171875,
    0.0145263671875,
    -0.004058837890625,
    0.0556640625,
    -0.01904296875,
    0.01220703125,
    0.03369140625,
    0.0164794921875,
    -0.017822265625,
    0.0311279296875,
    0.052734375,
    -0.0235595703125,
    0.0040283203125,
    -0.024658203125,
    0.005462646484375,
    -0.004608154296875,
    0.0306396484375,
    0.0115966796875,
    -0.0216064453125,
    -0.006134033203125,
    -0.0693359375,
    -0.0205078125,
    0.015869140625,
    -0.00079345703125,
    0.0654296875,
    -0.01531982421875,
    -0.00628662109375,
    0.0703125,
    -0.01190185546875,
    0.0098876953125,
    0.013916015625,
    -0.0185546875,
    -0.041259765625,
    -0.035888671875,
    -0.00799560546875,
    -0.0191650390625,
    -0.005523681640625,
    0.00762939453125,
    0.01513671875,
    0.06103515625,
    -0.023193359375,
    0.006805419921875,
    -0.034912109375,
    0.004425048828125,
    -0.0152587890625,
    0.00445556640625,
    -0.01336669921875,
    0.06396484375,
    -0.049072265625,
    0.01434326171875,
    0.01220703125,
    -0.004791259765625,
    -0.045166015625,
    0.05517578125,
    -0.02197265625,
    -0.056640625,
    -0.00021266937255859375,
    0.03662109375,
    -0.0634765625,
    -0.00848388671875,
    0.05029296875,
    0.06005859375,
    0.01312255859375,
    0.00689697265625,
    -0.0322265625,
    0.030517578125,
    0.013916015625,
    0.00109100341796875,
    -0.0146484375,
    -0.047607421875,
    0.0101318359375,
    -0.04736328125,
    0.05419921875,
    -0.0218505859375,
    0.0234375,
    0.01080322265625,
    -0.052978515625,
    0.0233154296875,
    0.005218505859375,
    -0.0242919921875,
    -0.0093994140625,
    0.000736236572265625,
    0.0198974609375,
    -0.0189208984375,
    -0.01287841796875,
    0.03466796875,
    0.0150146484375,
    -0.0126953125,
    -0.031982421875,
    0.02294921875,
    -0.0074462890625,
    -0.02880859375,
    0.00494384765625,
    0.02197265625,
    0.02734375,
    0.0106201171875,
    -0.01324462890625,
    0.00020694732666015625,
    -0.0281982421875,
    -0.0074462890625,
    -0.0308837890625,
    0.030029296875,
    0.050537109375,
    -0.0064697265625,
    -0.0084228515625,
    0.019287109375,
    0.0238037109375,
    0.03564453125,
    -0.07373046875,
    -0.0269775390625,
    -0.0126953125,
    -0.01446533203125,
    0.0177001953125,
    -0.044189453125,
    0.0009002685546875,
    0.0177001953125,
    0.0308837890625,
    0.05908203125,
    -0.0084228515625,
    0.022216796875,
    0.00152587890625,
    0.017822265625,
    -0.04541015625,
    0.04541015625,
    -0.006927490234375,
    0.08056640625,
    -0.0027313232421875,
    0.052490234375,
    0.0020904541015625,
    0.000037670135498046875,
    -0.040771484375,
    0.0048828125,
    0.0242919921875,
    0.0000896453857421875,
    -0.0322265625,
    -0.018310546875,
    0.003631591796875,
    -0.0018463134765625,
    -0.013427734375,
    -0.0230712890625,
    0.03466796875,
    0.00592041015625,
    -0.033935546875,
    0.02978515625,
    0.02978515625,
    0.02197265625,
    -0.0218505859375,
    -0.030029296875,
    0.0047607421875,
    0.0228271484375,
    0.0201416015625,
    0.003692626953125,
    0.005828857421875,
    0.00665283203125,
    0.06298828125,
    0.021728515625,
    -0.003265380859375,
    0.047119140625,
    -0.019775390625,
    -0.0166015625,
    -0.060546875,
    -0.055908203125,
    -0.0062255859375,
    0.0238037109375,
    0.01263427734375,
    -0.0093994140625,
    -0.0169677734375,
    -0.016357421875,
    -0.0126953125,
    0.0011138916015625,
    0.0103759765625,
    -0.0264892578125,
    0.018798828125,
    -0.00750732421875,
    -0.000682830810546875,
    0.0272216796875,
    0.03271484375,
    -0.0005950927734375,
    0.001861572265625,
    -0.01324462890625,
    0.003265380859375,
    -0.0185546875,
    0.00147247314453125,
    0.047119140625,
    -0.046630859375,
    0.01104736328125,
    0.013671875,
    -0.0031585693359375,
    -0.0191650390625,
    0.00262451171875,
    -0.00165557861328125,
    0.034423828125,
    -0.0031280517578125,
    0.02978515625,
    -0.00139617919921875,
    -0.0169677734375,
    0.02880859375,
    0.000713348388671875,
    -0.026123046875,
    -0.015869140625,
    -0.000213623046875,
    -0.0059814453125,
    -0.009521484375,
    -0.010009765625,
    -0.068359375,
    -0.0634765625,
    0.0074462890625,
    -0.01019287109375,
    0.03564453125,
    0.025390625,
    0.009521484375,
    -0.01080322265625,
    -0.007080078125,
    0.009033203125,
    0.0177001953125,
    -0.00125885009765625,
    -0.068359375,
    -0.00506591796875,
    -0.0145263671875,
    0.0023956298828125,
    -0.00848388671875,
    0.0196533203125,
    -0.00885009765625,
    0.00193023681640625,
    -0.02099609375,
    -0.021728515625,
    0.002471923828125,
    -0.0050048828125,
    0.021240234375,
    -0.01104736328125,
    0.0150146484375,
    -0.0159912109375,
    -0.0146484375,
    -0.0306396484375,
    0.0198974609375,
    0.040771484375,
    -0.01544189453125,
    0.022216796875,
    0.0169677734375,
    0.0213623046875,
    0.032470703125,
    -0.036376953125,
    -0.015869140625,
    -0.0089111328125,
    0.03662109375,
    0.0272216796875,
    0.03662109375,
    -0.02880859375,
    -0.0255126953125,
    0.02001953125,
    -0.0252685546875,
    0.047119140625,
    0.0130615234375,
    0.01220703125,
    0.0018157958984375,
    0.01263427734375,
    0.01422119140625,
    -0.018310546875,
    -0.0546875,
    -0.0027313232421875,
    0.0283203125,
    -0.07421875,
    -0.0069580078125,
    0.01080322265625,
    0.002716064453125,
    0.0118408203125,
    0.04052734375,
    0.0179443359375,
    -0.006011962890625,
    -0.04736328125,
    -0.01336669921875,
    0.0038909912109375,
    0.029541015625,
    -0.0341796875,
    -0.0240478515625,
    0.0235595703125,
    0.04931640625,
    0.0028228759765625,
    -0.041015625,
    -0.02197265625,
    0.0166015625,
    0.0302734375,
    -0.0196533203125,
    0.03466796875,
    0.00494384765625,
    -0.00160980224609375,
    -0.029541015625,
    0.06689453125,
    0.03369140625,
    -0.00982666015625,
    0.0166015625,
    0.01104736328125,
    0.039794921875,
    0.04931640625,
    0.0020751953125,
    0.014404296875,
    0.03173828125,
    0.003082275390625,
    -0.0238037109375,
    0.023681640625,
    0.033203125,
    0.026123046875,
    -0.005401611328125,
    -0.023193359375,
    0.037353515625,
    -0.00628662109375,
    -0.046142578125,
    0.0218505859375,
    -0.01806640625,
    0.002777099609375,
    -0.029541015625,
    -0.0003948211669921875,
    0.03662109375,
    0.004669189453125,
    -0.00457763671875,
    -0.0283203125,
    -0.007110595703125,
    -0.049072265625,
    0.0157470703125,
    -0.0269775390625,
    -0.033935546875,
    0.005889892578125,
    -0.005859375,
    0.051513671875,
    -0.034423828125,
    -0.00701904296875,
    -0.00494384765625,
    0.0089111328125,
    0.02490234375,
    -0.01263427734375,
    0.0125732421875,
    -0.0054931640625,
    -0.01458740234375,
    0.0167236328125,
    -0.0216064453125,
    0.0135498046875,
    0.0050048828125,
    0.0166015625,
    -0.00165557861328125,
    -0.0059814453125,
    -0.0106201171875,
    -0.0201416015625,
    0.0003414154052734375,
    -0.01300048828125,
    0.039306640625,
    -0.0130615234375,
    0.005401611328125,
    -0.005035400390625,
    -0.02685546875,
    0.009521484375,
    0.00201416015625,
    -0.01177978515625,
    -0.0017242431640625,
    0.014892578125,
    -0.051025390625,
    0.033203125,
    -0.00750732421875,
    0.006561279296875,
    0.0128173828125,
    0.0185546875,
    0.00994873046875,
    -0.029541015625,
    -0.0311279296875,
    0.0291748046875,
    -0.007659912109375,
    -0.0279541015625,
    0.0262451171875,
    0.0201416015625,
    0.00145721435546875,
    0.017333984375,
    -0.01080322265625,
    -0.022705078125,
    0.0218505859375,
    0.0028076171875,
    -0.015869140625,
    -0.0106201171875,
    -0.0240478515625,
    0.037109375,
    0.00372314453125,
    0.02490234375,
    -0.055419921875,
    0.03466796875,
    0.0216064453125,
    -0.009521484375,
    -0.043701171875,
    -0.0023651123046875,
    0.01019287109375,
    0.02099609375,
    0.03466796875,
    -0.011962890625,
    -0.033935546875,
    -0.044189453125,
    -0.01300048828125,
    -0.00012969970703125,
    0.00046539306640625,
    -0.0252685546875,
    -0.004669189453125,
    0.02978515625,
    0.01556396484375,
    0.0020751953125,
    -0.033203125,
    -0.000438690185546875,
    0.03662109375,
    -0.0238037109375,
    0.047607421875,
    -0.00830078125,
    -0.045166015625,
    0.00555419921875,
    0.02978515625,
    -0.011962890625,
    -0.006744384765625,
    0.0115966796875,
    0.03369140625,
    -0.0003490447998046875,
    -0.02880859375,
    0.007781982421875,
    0.01263427734375,
    -0.034912109375,
    -0.0184326171875,
    0.017578125,
    -0.05224609375,
    -0.033447265625,
    0.02783203125,
    0.0029296875,
    0.019775390625,
    -0.0010986328125,
    0.037109375,
    -0.01043701171875,
    -0.01348876953125,
    -0.0098876953125,
    -0.0286865234375,
    -0.0052490234375,
    -0.0167236328125,
    0.01385498046875,
    0.0038299560546875,
    0.00080108642578125,
    0.0150146484375,
    -0.01116943359375,
    0.02685546875,
    -0.006927490234375,
    -0.0244140625,
    -0.01165771484375,
    0.00665283203125,
    -0.02294921875,
    0.030029296875,
    0.0115966796875,
    0.04248046875,
    -0.00860595703125,
    -0.019775390625,
    -0.0242919921875,
    -0.00634765625,
    -0.056396484375,
    -0.01531982421875,
    -0.0157470703125,
    0.005035400390625,
    -0.012451171875,
    -0.00567626953125,
    -0.033447265625,
    0.00653076171875,
    0.00689697265625,
    0.0096435546875,
    0.0250244140625,
    0.00151824951171875,
    -0.00714111328125,
    -0.01129150390625,
    0.0147705078125,
    0.031982421875,
    -0.00506591796875,
    -0.01055908203125,
    0.0096435546875,
    -0.02490234375,
    0.01220703125,
    0.019775390625,
    0.0152587890625,
    -0.01226806640625,
    -0.0038299560546875,
    -0.02978515625,
    -0.020263671875,
    -0.00008630752563476562,
    -0.005706787109375,
    0.061767578125,
    -0.0113525390625,
    0.02880859375,
    -0.004913330078125,
    -0.0177001953125,
    -0.030029296875,
    0.00909423828125,
    -0.004425048828125,
    -0.00013065338134765625,
    0.01129150390625,
    0.03955078125,
    -0.01318359375,
    0.009765625,
    -0.037353515625,
    0.041748046875,
    0.0012969970703125,
    0.000507354736328125,
    -0.0201416015625,
    0.01165771484375,
    -0.013427734375,
    0.0064697265625,
    -0.03173828125,
    -0.0174560546875,
    -0.01007080078125,
    -0.004913330078125,
    -0.0255126953125,
    0.021240234375,
    -0.03662109375,
    -0.00032806396484375,
    0.0162353515625,
    -0.01458740234375,
    0.0030670166015625,
    -0.01806640625,
    0.0023193359375,
    -0.007720947265625,
    -0.036376953125,
    -0.00408935546875,
    0.00762939453125,
    0.0166015625,
    0.0013427734375,
    -0.01068115234375,
    -0.005645751953125,
    -0.00921630859375,
    -0.0225830078125,
    -0.028564453125,
    -0.01434326171875,
    0.040283203125,
    -0.0400390625,
    0.0030059814453125,
    0.040771484375,
    -0.005218505859375,
    -0.01007080078125,
    0.0400390625,
    0.034912109375,
    0.0263671875,
    -0.0419921875,
    -0.023681640625,
    0.0089111328125,
    -0.00061798095703125,
    -0.027587890625,
    -0.00077056884765625,
    0.0166015625,
    0.046142578125,
    0.0014801025390625,
    -0.0238037109375,
    0.01531982421875,
    -0.0023956298828125,
    0.00982666015625,
    0.001800537109375,
    0.01043701171875,
    -0.00885009765625,
    -0.00408935546875,
    -0.01806640625,
    -0.005340576171875,
    -0.025634765625,
    -0.0230712890625,
    -0.001678466796875,
    -0.047119140625,
    0.0012054443359375,
    0.00714111328125,
    -0.01806640625,
    -0.03271484375,
    0.0162353515625,
    0.0027313232421875,
    0.0004367828369140625,
    -0.0498046875,
    -0.0098876953125,
    0.049072265625,
    0.006195068359375,
    0.0162353515625,
    -0.0191650390625,
    0.03173828125,
    -0.019287109375,
    0.01513671875,
    0.0169677734375,
    0.017333984375,
    0.01019287109375,
    0.000957489013671875,
    0.021728515625,
    -0.033447265625,
    0.0025634765625,
    0.001068115234375,
    0.00156402587890625,
    -0.01458740234375,
    -0.0537109375,
    0.0113525390625,
    0.0218505859375,
    -0.03662109375,
    -0.0262451171875,
    0.01129150390625,
    -0.01300048828125,
    -0.03955078125,
    0.0216064453125,
    0.009521484375,
    -0.00799560546875,
    -0.0093994140625,
    -0.0133056640625,
    -0.01251220703125,
    -0.00738525390625,
    -0.00830078125,
    0.004547119140625,
    -0.00860595703125,
    -0.004608154296875,
    -0.043212890625,
    0.020263671875,
    0.01141357421875,
    -0.01458740234375,
    0.028564453125,
    -0.016845703125,
    0.00159454345703125,
    -0.0341796875,
    -0.02880859375,
    0.0306396484375,
    0.0137939453125,
    -0.03466796875,
    0.02734375,
    -0.0169677734375,
    0.0262451171875,
    0.038330078125,
    -0.04296875,
    0.047119140625,
    -0.0145263671875,
    -0.000789642333984375,
    0.00811767578125,
    0.0030364990234375,
    0.006927490234375,
    0.0341796875,
    0.038330078125,
    0.00433349609375,
    -0.014892578125,
    -0.00762939453125,
    -0.0052490234375,
    -0.00732421875,
    0.0234375,
    -0.00081634521484375,
    0.019287109375,
    0.00372314453125,
    0.0206298828125,
    -0.0218505859375,
    -0.0091552734375,
    0.032470703125,
    0.0145263671875,
    -0.000881195068359375,
    0.02587890625,
    -0.0201416015625,
    0.00933837890625,
    0.018310546875,
    0.018310546875,
    0.02392578125,
    0.005889892578125,
    0.0031585693359375,
    -0.009521484375,
    0.0311279296875,
    -0.0308837890625,
    -0.0019073486328125,
    0.0196533203125,
    -0.004608154296875,
    0.0263671875,
    0.0203857421875,
    0.01324462890625,
    -0.00787353515625,
    -0.054443359375,
    -0.006683349609375,
    0.003021240234375,
    0.005279541015625,
    0.0142822265625,
    0.0150146484375,
    -0.00634765625,
    0.01220703125,
    -0.0255126953125,
    0.0162353515625,
    0.0184326171875,
    -0.00031280517578125,
    -0.01031494140625,
    0.0074462890625,
    -0.06689453125,
    0.005218505859375,
    0.012451171875,
    0.0177001953125,
    -0.048828125,
    -0.01458740234375,
    -0.006317138671875,
    0.0142822265625,
    -0.0250244140625,
    0.002349853515625
  ],
  "created": "2025-06-12T05:14:14.234978",
  "count": 45
}